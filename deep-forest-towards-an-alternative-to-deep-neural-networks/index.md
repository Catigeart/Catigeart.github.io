# 【论文笔记】Deep Forest - Towards an alternative to deep neural networks


目前的深度学习模型大多建立在神经网络上，即多层参数化的可微非线性模块，可以通过反向传播进行训练。在本文中，我们探索了基于不可微模块构建深度模型的可能性。我们推测深度神经网络成功背后的奥秘在很大程度上归功于三个特征，即逐层处理、模型内特征转换和足够的模型复杂性。我们提出了gcForest方法，该方法生成了具有这些特征的深度森林。这是一种决策树集成方法，具有比深度神经网络少得多的超参数，并且其模型复杂性可以以数据依赖的方式自动确定。实验表明，该算法的性能对超参数设置具有很强的鲁棒性，因此在大多数情况下，即使跨不同领域的不同数据，使用相同的默认设置也能获得优异的性能。本研究打开了基于不可微模块的深度学习的大门，并展示了不使用反向传播构建深度模型的可能性。

Zhou Z H, Feng J. Deep Forest: Towards An Alternative to Deep Neural Networks[C]//IJCAI. 2017: 3553-3559.

## 1 介绍

DNN的弊端：

- 超参数过多，且超参数的选取也很难理论分析
- 理论分析困难
- 所需数据量大
- 黑盒模型难以理解角色过程
- 训练前必须确定神经网络的结构，不能以数据依赖的方式自动确定
- 一些任务中DNN表现不优越：Kaggle许多竞赛任务的赢家仍然是随机森林和XGBoost

本文致力于解决的问题：**深度学习可以在不可微的模块上实现吗？**

- => 深度模型是否=DNN？
- => 有没有可能训练没有反向传播的深度模型？（反向传播需要可微性）
- => 是否有可能使深度模型赢得现在其他模型（如随机森林和XGBoost）更好的任务？

**gcForest**：非NN风格的深度模型，具有级联结构的决策树集成，以数据依赖的方式确定模型复杂性，超参数少，表现稳健

## 2 动机

DNN的启示：

- 深层网络的表征学习能力比浅层网络强 => **逐层处理**

- 决策树和boost也执行逐层处理，但表现不佳 => 没有**模型内特征转换**，限制了**模型复杂度**

## 集成学习的启示：

- $E=\overline{E}-\overline{A}$：个体分类器越准确、越多样化，整体效果越好

- 多样性增强策略：
  1. 数据样本操作
  2. 输入特征操作
  3. 学习参数操作
  4. 输出表示操作

## 3 gcForest方法

### 3.1 级联森林结构

![image-20230509222625870](https://cdn.jsdelivr.net/gh/Catigeart/imgHost/img/dl/image-20230509222625870.png)

![image-20230509222655068](https://cdn.jsdelivr.net/gh/Catigeart/imgHost/img/dl/image-20230509222655068.png)

特征增强仅采用了最简单的类向量的形式，如此少量的特征增强可能非常有限，但实验证明这样简单的特征增强也是有益的。进一步的可能性留给未来探索。

为了减少过拟合的风险，每个森林产生的类向量通过k折验证生成。若验证集估计增加层数后的整个级联的性能没有显著性能增益，则训练过程自动终止。

### 3.2 多粒度扫描

![image-20230509223722265](https://cdn.jsdelivr.net/gh/Catigeart/imgHost/img/dl/image-20230509223722265.png)

![image-20230509223823468](https://cdn.jsdelivr.net/gh/Catigeart/imgHost/img/dl/image-20230509223823468.png)

## 4 实验

- 4个完全随机树森林和4个随机森林，每个随机森林包含500棵树，3折验证
- 80%的数据用于生长集，20%的数据用于估计集
- 使用三种不同尺寸的特征窗口：$\lfloor d/16\rfloor,\lfloor d/8\rfloor,\lfloor d/4\rfloor$

多个数据集表现出色，对级联、多粒度和模型规模对结果的影响进行了分析。

## 5 未来的工作

- 特征增强
- 加快速度、减小内存
- 结合主动学习和半监督学习
