[{"categories":["个人笔记"],"content":"Attention Is All You Need 主要的序列转导模型是基于复杂的循环或卷积神经网络，包括一个编码器和一个解码器。表现最好的模型还通过注意机制连接编码器和解码器。我们提出了一个新的简单的网络架构，变压器，完全基于注意力机制，完全摒弃递归和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时更具并行性，并且需要更少的训练时间。我们的模型在WMT 2014英语-德语翻译任务上实现了28.4 BLEU，比现有的最佳结果(包括集合)提高了2个BLEU以上。在WMT 2014英法翻译任务中，我们的模型在8个gpu上训练3.5天后，建立了一个新的单模型最先进的BLEU分数41.8，这是文献中最佳模型训练成本的一小部分。我们通过将Transformer成功地应用于具有大量和有限训练数据的英语选区解析，证明了它可以很好地推广到其他任务。 Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30. ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:0:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"1 Intro RNN、LSTM、门RNN已经是state of the art的序列模型 encoder-decoder架构 循环模型因为其顺序性质，阻碍了并行化 注意力机制允许不考虑序列中的距离建模依赖关系，大部分与RNN结合使用 Transformer：完全依赖注意力机制 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:1:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"2 Background 已有的避免序列依赖计算的方法采用卷积神经网络，然而计算任意位置关联信息的算法复杂度大（线性、常数）。Transformer采用常数复杂度的计算，用多头注意力机制弥补因此带来的分辨能力下降的问题 自注意力机制：将单个序列不同位置联系起来的注意机制 端到端记忆网络基于循环注意力机制，而不是序列对齐的循环 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:2:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"3 Model Arch ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:3:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"3.1 Encoder-Decoder $$ (x_1,x_2,…,x_n)\\rightarrow(z_1,z_2,…,z_n)\\rightarrow(y_1,y_2….,y_m) $$ 从一个序列映射到另一个序列，每步都是自回归的，所生成的标志作为下一步的额外输入。 Encoder: 编码器由N = 6个相同层的堆栈组成。每一层有两个子层。 第一个是一个多头自注意机制， 第二个是一个简单的、按位置完全连接的前馈网络。 我们在每两个子层周围使用一个残差，然后进行层归一化。也就是说，每个子层的输出是LayerNorm(x + subblayer (x))，其中subblayer (x)是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生维度为dmodel = 512的输出。 Decoder: 解码器也由N = 6个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。 与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。 我们还修改了解码器堆栈中的自注意力子层，以防止注意到后续的位置。这种掩码，结合输出嵌入被一个位置抵消的事实，确保对位置$i$的预测只能依赖于小于$i$位置的已知输出。 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:3:1","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"3.2 Attention $$ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ 加性注意力理论复杂度与点乘注意力相似，但点乘可依靠高度优化的矩阵乘法加速 $d_k$较大时点乘注意力表现不佳，因此予以放缩 多头注意力： $$ MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O \\ where\\ head_i=Attention(QW_i^Q,KW_i^K,VW_i^V) $$ 允许模型在不同位置共同注意来自不同表示子空间的信息 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:3:2","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"4 为什么使用自注意力： 易于并行化 与RNN相比，易于学习远程依赖关系 与CNN相比，一般大小的卷积层不能连接所有输入和输出的位置，若扩展卷积则计算复杂度巨大 自注意力机制提供可解释性 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:4:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"5 训练 标准 WMT 2014 英语-德语数据集 8块 NVIDIA P100 GPU Adam 优化器：$\\beta _1=0.9,\\beta _2=0.98,\\epsilon=10 ^{-9}$ $$ lrate=d_{model}^{-0.5}·\\min (step_num ^{-0.5},step_num·warmup_steps ^{-1.5}) $$ 正则化：残差dropout，标签平滑化 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:5:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"6 结果：新的 state-of-the-art ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:6:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"7 结论： 提出了Transformer，在翻译任务取得了state-of-the-art，下一步工作是对注意力机制进行扩展 ","date":"2023-05-30","objectID":"/attention-is-all-you-need/:7:0","tags":["个人笔记"],"title":"【论文简记】Attention Is All You Need","uri":"/attention-is-all-you-need/"},{"categories":["个人笔记"],"content":"Deep Forest - Towards an alternative to deep neural networks 目前的深度学习模型大多建立在神经网络上，即多层参数化的可微非线性模块，可以通过反向传播进行训练。在本文中，我们探索了基于不可微模块构建深度模型的可能性。我们推测深度神经网络成功背后的奥秘在很大程度上归功于三个特征，即逐层处理、模型内特征转换和足够的模型复杂性。我们提出了gcForest方法，该方法生成了具有这些特征的深度森林。这是一种决策树集成方法，具有比深度神经网络少得多的超参数，并且其模型复杂性可以以数据依赖的方式自动确定。实验表明，该算法的性能对超参数设置具有很强的鲁棒性，因此在大多数情况下，即使跨不同领域的不同数据，使用相同的默认设置也能获得优异的性能。本研究打开了基于不可微模块的深度学习的大门，并展示了不使用反向传播构建深度模型的可能性。 Zhou Z H, Feng J. Deep Forest: Towards An Alternative to Deep Neural Networks[C]//IJCAI. 2017: 3553-3559. ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"1 介绍 DNN的弊端： 超参数过多，且超参数的选取也很难理论分析 理论分析困难 所需数据量大 黑盒模型难以理解角色过程 训练前必须确定神经网络的结构，不能以数据依赖的方式自动确定 一些任务中DNN表现不优越：Kaggle许多竞赛任务的赢家仍然是随机森林和XGBoost 本文致力于解决的问题：深度学习可以在不可微的模块上实现吗？ =\u003e 深度模型是否=DNN？ =\u003e 有没有可能训练没有反向传播的深度模型？（反向传播需要可微性） =\u003e 是否有可能使深度模型赢得现在其他模型（如随机森林和XGBoost）更好的任务？ gcForest：非NN风格的深度模型，具有级联结构的决策树集成，以数据依赖的方式确定模型复杂性，超参数少，表现稳健 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"2 动机 DNN的启示： 深层网络的表征学习能力比浅层网络强 =\u003e 逐层处理 决策树和boost也执行逐层处理，但表现不佳 =\u003e 没有模型内特征转换，限制了模型复杂度 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:2:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"集成学习的启示： $E=\\overline{E}-\\overline{A}$：个体分类器越准确、越多样化，整体效果越好 多样性增强策略： 数据样本操作 输入特征操作 学习参数操作 输出表示操作 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:3:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"3 gcForest方法 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:4:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"3.1 级联森林结构 特征增强仅采用了最简单的类向量的形式，如此少量的特征增强可能非常有限，但实验证明这样简单的特征增强也是有益的。进一步的可能性留给未来探索。 为了减少过拟合的风险，每个森林产生的类向量通过k折验证生成。若验证集估计增加层数后的整个级联的性能没有显著性能增益，则训练过程自动终止。 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:4:1","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"3.2 多粒度扫描 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:4:2","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"4 实验 4个完全随机树森林和4个随机森林，每个随机森林包含500棵树，3折验证 80%的数据用于生长集，20%的数据用于估计集 使用三种不同尺寸的特征窗口：$\\lfloor d/16\\rfloor,\\lfloor d/8\\rfloor,\\lfloor d/4\\rfloor$ 多个数据集表现出色，对级联、多粒度和模型规模对结果的影响进行了分析。 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:5:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"5 未来的工作 特征增强 加快速度、减小内存 结合主动学习和半监督学习 ","date":"2023-05-30","objectID":"/deep-forest-towards-an-alternative-to-deep-neural-networks/:6:0","tags":["个人笔记"],"title":"【论文简记】Deep Forest - Towards an alternative to deep neural networks","uri":"/deep-forest-towards-an-alternative-to-deep-neural-networks/"},{"categories":["个人笔记"],"content":"Deep Residual Learning for Image Recognition 深度神经网络更难训练。我们提出了一个残差学习框架，以简化比以前使用的网络深度大得多的网络的训练。我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从相当大的深度中获得精度。在ImageNet数据集上，我们评估了深度高达152层的残差网络——比VGG网络深度8倍[40]，但仍然具有较低的复杂性。这些残差网络的集合在ImageNet测试集上的误差达到3.57%。该结果在ILSVRC 2015分类任务中获得第一名。我们还介绍了100层和1000层的CIFAR-10分析。 表征的深度对于许多视觉识别任务至关重要。仅由于我们的极深表示，我们在COCO对象检测数据集上获得了28%的相对改进。深度残差网络是我们参加ILSVRC \u0026 COCO 2015竞赛的基础1，我们还在ImageNet检测、ImageNet定位、COCO检测和COCO分割的任务中获得了第一名。 He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778. ","date":"2023-05-30","objectID":"/deep-residual-learning-for-image-recognition/:0:0","tags":["个人笔记"],"title":"【论文简记】Deep Residual Learning for Image Recognition","uri":"/deep-residual-learning-for-image-recognition/"},{"categories":["个人笔记"],"content":"1 深度网络的训练问题 多个模型证明了更深的模型有取得更好表现的潜力。然而，随着网络的加深，训练却出现了退化现象。即网络加深了，精度却下降了，而且不是由过拟合，也不是梯度消失/爆炸引起的。 理论上，更深的网络不应该比更浅的网络差。出现这种现象是因为深层次的网络优化越来越困难。 ","date":"2023-05-30","objectID":"/deep-residual-learning-for-image-recognition/:1:0","tags":["个人笔记"],"title":"【论文简记】Deep Residual Learning for Image Recognition","uri":"/deep-residual-learning-for-image-recognition/"},{"categories":["个人笔记"],"content":"2 深度残差网络 核心假设：通过一个堆叠的非线性层去拟合（残差）更容易 该假设的动机：更深的网络不应该表现比更浅的网络差，因为在极端情况下，更浅的网络继续叠加恒等映射使其变成更深的网络，也不会降低效果。因此顺着恒等映射的思路出发，如果恒等映射是最优的，那么通过残差块，可以轻易地将其跳过的模型块的参数逼近0，从而逼近恒等映射 $$ \\bold{y}=\\mathcal{F}(\\bold{x},{W_i})+\\bold{x} $$ 没有引入额外的参数和计算复杂度 ","date":"2023-05-30","objectID":"/deep-residual-learning-for-image-recognition/:2:0","tags":["个人笔记"],"title":"【论文简记】Deep Residual Learning for Image Recognition","uri":"/deep-residual-learning-for-image-recognition/"},{"categories":["个人笔记"],"content":"3 ResNet 模型实验： VGG 以VGG为基础网络，减少了每层的参数，增加了层的数量； 上述的基础上增加短路连接 观察到退化现象，增加短路连接效果更好 =\u003e ResNet ","date":"2023-05-30","objectID":"/deep-residual-learning-for-image-recognition/:3:0","tags":["个人笔记"],"title":"【论文简记】Deep Residual Learning for Image Recognition","uri":"/deep-residual-learning-for-image-recognition/"},{"categories":["个人笔记"],"content":"Graph attention networks Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017. 我们提出了图注意网络(GATs)，这是一种新颖的神经网络架构，可以在图结构数据上运行，利用隐藏的自注意层来解决基于图卷积或其近似的先前方法的缺点。通过堆叠层，其中的节点能够参与其邻居的特征，我们可以(隐式地)为邻居中的不同节点指定不同的权重，而不需要任何昂贵的矩阵操作(例如反转)或依赖于预先知道的图结构。 通过这种方式，我们同时解决了基于频谱的图神经网络的几个关键挑战，并使我们的模型很容易适用于感应和转导问题。我们的GAT模型已经在四个已建立的传导和归纳图基准中达到或匹配了最先进的结果:Cora, Citeseer和Pubmed引文网络数据集，以及蛋白质-蛋白质相互作用数据集(其中测试图在训练期间保持不可见)。 ","date":"2023-05-30","objectID":"/graph-attention-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Graph attention networks","uri":"/graph-attention-networks/"},{"categories":["个人笔记"],"content":"GAT结构 GA层的输入是一组节点特征： $$ \\bold{h}={\\vec{h}_1,\\vec{h}_2,…,\\vec{h}_N},\\vec{h}_i\\in \\mathbb{R}^F $$ 其中，$N$是节点的数量，$F$是每个节点的特征数量。GA层的输出是一组新的节点特征： $$ \\bold{h}’={\\vec{h}_1’,\\vec{h}_2’,…,\\vec{h}_N’},\\vec{h}_i’\\in \\mathbb{R}^{F’} $$ 参数含义同理。 为了学习到更高层的特征，引入可学习的线性变换。每个节点应用共享线性变换： $$ e_{ij}=a(\\bold{W}\\vec{h_i},\\bold{W}\\vec{h_j}) $$ 这表示出了节点j对节点i的重要性。最一般情况下，所有节点都可以注意到每一个节点，但是这种做法会丢失图的结构信息，因此选择只关注一阶邻居，称为带掩码的图注意力。 为了使不同节点的系数易于比较，在本文实验中，用softmax将其归一化： $$ \\alpha_{ij}=softmax_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}i}\\exp(e{ik})} $$ 本文实验中，函数$a$实例化如下，并得到对应的$\\alpha_{ij}$式子： $$ {\\rm LeakyReLU}(\\bold{\\vec{a}}^T[\\bold{W}\\vec{h_i}||\\bold{W}\\vec{h_j}]) $$ $$ \\alpha_{ij}=\\frac{\\exp({\\rm LeakyReLU}(\\bold{\\vec{a}}^T[\\bold{W}\\vec{h_i}||\\bold{W}\\vec{h_j}]))}{\\sum_{k\\in\\mathcal{N}_i}\\exp({\\rm LeakyReLU}(\\bold{\\vec{a}}^T[\\bold{W}\\vec{h_i}||\\bold{W}\\vec{h_k}]))} $$ 在得到归一化的注意力系数之后，可以通过对邻接节点特征的线性组合经过一个非线性激活函数来更新节点自身的特征作为输出： $$ \\vec{h_i’}=\\sigma(\\sum_{j\\in \\mathcal{N}i}\\alpha_{ij}\\bold{W}\\vec{h_j}) $$ 进一步，为了稳定注意力效果，采取多头注意力，将计算后的结果Concat： $$ \\vec{h_i’}\\Vert_{k=1}^K\\sigma(\\sum_{j\\in\\mathcal{N}i}\\alpha{ij}^k\\bold{W}^k\\vec{h_j}) $$ 其中双竖线代表拼接。 如果我们在最后一层（预测层）使用Multi-head attention mechanism，需要把输出进行平均化，再使用非线性函数（softmax或logistic sigmoid）： $$ \\vec{h_i’}=\\sigma(\\frac{1}{K}\\sum_{k=1}^K\\sum_{j\\in\\mathcal{N}i}\\alpha{ij}^k\\bold{W}^k\\vec{h_j}) $$ GA的优点： 计算效率高，跨节点对可并行计算 可以隐式地为同一邻域的节点分配不同的重要性，也有助于模型可解释性 注意机制以共享的方式应用于图中的所有边，因此它不依赖于对全局图结构或所有节点(特征)的预先访问 ","date":"2023-05-30","objectID":"/graph-attention-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Graph attention networks","uri":"/graph-attention-networks/"},{"categories":["个人笔记"],"content":"评估 补充解释： Transductive learning：训练阶段与测试阶段都基于同样的图结构 Inductive learning：训练阶段与测试阶段需要处理的图不同。通常训练阶段只是在子图（subgraph）上进行，测试阶段需要处理未知的顶点（unseen nodes） 实验结果： ","date":"2023-05-30","objectID":"/graph-attention-networks/:2:0","tags":["个人笔记"],"title":"【论文简记】Graph attention networks","uri":"/graph-attention-networks/"},{"categories":["个人笔记"],"content":"Modeling Relational Data with Graph Convolutional Networks Schlichtkrull M, Kipf T N, Bloem P, et al. Modeling relational data with graph convolutional networks[C]//The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3–7, 2018, Proceedings 15. Springer International Publishing, 2018: 593-607. 知识图支持各种各样的应用，包括问题回答和信息检索。尽管在它们的创建和维护上投入了巨大的努力，但即使是最大的(例如，Yago、DBPedia或Wikidata)也仍然不完整。我们引入了关系图卷积网络(R-GCNs)，并将其应用于两个标准的知识库补全任务:链接预测(缺失事实的恢复，即主-谓词-对象三元组)和实体分类(缺失实体属性的恢复)。RGCNs与最近一类在图上操作的神经网络相关，并且专门用于处理现实知识库的高度多关系数据特征。我们证明了R-GCNs作为实体分类的独立模型的有效性。我们进一步证明，用于链路预测的分解模型(如DistMult)可以通过使用编码器模型来丰富它们，从而在关系图中的多个推理步骤中积累证据，从而得到显著改进，证明在FB15k-237上比仅解码器基线提高了29.8%。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"1 Intro 知识库组织和存储知识，但存在知识完整性的问题，从而产生了预测知识库中的缺失信息的统计关系学习（SRL）任务。假设知识库存储三元组：主语、谓词、宾语，那么可以建立有向二元关系。考虑两个SRL任务：链路预测（恢复丢失的三元组）和实体分类（为实体分配类型或分类属性）。 本文贡献： 第一个证明图卷积网络（GCN）框架可应用于关系数据建模，特别是链路预测和实体分类 引入参数共享和强制稀疏约束的技术，并利用他们将R-GCNs应用于具有大量关系的多图 展示因式分解模型的性能，以DistMult为例 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"2 神经关系建模 本文符号：$G=(\\mathcal{V,E,R}), v_i\\in V, (v_i, r,v_j)\\in \\mathcal{E},r\\in\\mathcal{R}$ ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:2:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"2.1 关系图卷积网络 我们的模型是GCNs模型的扩展，可以理解为可微消息传递框架的特例： $$ h_i^{l+1}=\\sigma(\\sum_{m\\in\\mathcal{M}_i}g_m(h_i^{(l)},h_j^{(l)})) $$ 其中$h_i^{(l)}\\in\\mathbb{R}^{d^{(l)}}$是节点$v_i$在第i层隐藏层的状态，$d^{(l)}$是该层的维度。$g_m$是传递的消息，$\\sigma$是激活函数，$\\mathcal{M}_i$是该结点的输入消息集。 受其启发，我们在带标签有向图之上的前向传播模型如下： $$ h_i^{(l+1)}=\\sigma(\\sum_{r\\in\\mathcal{R}}\\sum_{j\\in\\mathcal{N}i^r}\\frac{1}{c{i,r}}W_r^{(l)}h_j^{(l)}+W_0^{(l)}h_i^{(l)}) $$ $\\mathcal{N}i^r$表示关系R中节点i的邻居索引集，$c{i,r}$是特定问题的归一化系数，可以学习得到或提前指定。 直观上，上式通过归一化和对相邻节点变换后的特征向量进行累加。与常规GCNs不同，我们引入了特定于关系的转换，即依赖于边缘的类型和方向。 为了确保l + 1层节点的表示也可以被l层对应的表示所告知，我们为数据中的每个节点添加了一个特殊关系类型的单个自连接。请注意，可以选择更灵活的函数，如多层神经网络(以牺牲计算效率为代价)，而不是简单的线性消息转换。我们把这个留给以后的工作。 通过堆叠上述块构建R-GCNs。 上式可以用稀疏矩阵有效实现，可以通过层的堆叠学习多个关系步骤的依赖。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:2:1","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"2.2 正则化 将R-GCNs应用于高度多关系数据的一个中心问题是，随着图中关系数量的增加，参数数量也会迅速增加。在实践中，这很容易导致对稀有关系的过度拟合和非常大的模型。 文章引入了两种不同的正则化方法：基分解(basis-decomposition)和块对角分解(block-diagonal-decomposition)。 基分解将$W_r^{(l)}$分解为B个基变换的线性组合： $$ W_r{(l)}=\\sum_{b=1}^B a_{rb}^{(l)}V_b^{(l)} $$ 块对角分解将$W_r^{(l)}$视为块对角矩阵： $$ W_r^{(l)}=\\oplus_{b=1}^B Q_{br}^{(l)} $$ 基函数分解可以看作是不同关系类型之间有效的权值共享形式，而块分解可以看作是对每个关系类型的权值矩阵的稀疏性约束。块分解结构编码了一种直觉，即潜在特征可以分组为变量集，这些变量集在组内比组间耦合得更紧密。这两种分解都减少了学习高度多关系数据(如现实知识库)所需的参数数量。同时，我们期望基参数化可以缓解稀有关系的过拟合，因为在稀有关系和更频繁的关系之间共享参数更新。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:2:2","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"3 实体分类 对于节点(实体)的(半)监督分类，我们简单地将形式为(2)的R-GCN层堆叠起来，在最后一层的输出上使用softmax(·)激活(每个节点)。我们最小化所有标记节点上的以下交叉熵损失(忽略未标记节点): $$ \\mathcal{L}=-\\sum_{i\\in\\mathcal{Y}}\\sum_{k=1}^K t_{ik}\\ln h_{ik} ^{(L)} $$ $\\mathcal{Y}$是有标签的节点索引集，$h_{ik}^{(L)}$是第i个带标签节点的第k个输出，$t_{ik}$是真值标签。使用批梯度下降训练模型。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:3:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"4 链路预测 链接预测处理的是对新事实的预测，如三元组(主语，关系，宾语)。形式上，知识库由一个有向标记图G = (V, E, R)表示。我们得到的不是边的完整集合E，而是不完全子集E。任务是给可能的边(s, r, o)分配分数f(s, r, o)，以确定这些边属于E的可能性有多大。 因此，本文引入图自编码器模型，由实体编码器（R-GCNs）和打分函数（解码器）组成。 本文使用DistMult作为打分函数： $$ f(s,r,o)=e_s^TR_re_o $$ 其中$e_s$和$e_o$是使用R-GCN学习到的实体嵌入，$R_r$是可学习的关系r对应的对角矩阵。 使用负采样训练模型。对每个正样本采样$\\omega$个负样本通过优化交叉熵损失函数使模型给正样本的打分高于负样本： $$ \\mathcal{L}=-\\frac{1}{(1+\\omega)|\\hat{\\mathcal{E}}|}\\sum_{(s,r,o,y)\\in T}y\\log l(f(s,r,o))+(1-y)\\log (1-l(f(s,r,o))) $$ 其中T是正样本和负样本集合，l是sigmoid函数，y=1表示正样本，y=0表示负样本。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:4:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"5 实验 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:5:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"5.1 实体分类 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:5:1","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"5.2 链路预测 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:5:2","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"结论 我们介绍了关系图卷积网络(R-GCNs)，并在两个标准统计关系建模问题的背景下证明了它们的有效性：链路预测和实体分类。对于实体分类问题，我们已经证明了R-GCN模型可以作为一个竞争性的，端到端可训练的基于图的编码器。对于链路预测，以DistMult分解为解码组件的R-GCN模型优于直接优化的分解模型，在标准链路预测基准上取得了具有竞争力的结果。使用RGCN编码器丰富因子分解模型对于具有挑战性的FB15k-237数据集特别有价值，比仅解码器基线提高29.8%。 我们的工作可以通过几种方式得到扩展。例如，图自编码器模型可以与其他分解模型结合使用，例如ComplEx (Trouillon et al . 2016)，它可以更适合于非对称关系的建模。在R-GCNs中集成实体特征也很简单，这将有利于链路预测和实体分类问题。为了解决我们方法的可扩展性，探索子采样技术是值得的，例如Hamilton, Ying和Leskovec(2017)。最后，它有望用数据依赖的注意机制取代当前对相邻节点和关系类型的求和形式。除了建模知识库之外，R-GCNs还可以推广到关系分解模型已经被证明有效的其他应用中。 ","date":"2023-05-30","objectID":"/modeling-relational-data-with-graph-convolutional-networks/:6:0","tags":["个人笔记"],"title":"【论文简记】Modeling Relational Data with Graph Convolutional Networks","uri":"/modeling-relational-data-with-graph-convolutional-networks/"},{"categories":["个人笔记"],"content":"Neural Message Passing for Quantum Chemistry Gilmer J, Schoenholz S S, Riley P F, et al. Neural message passing for quantum chemistry[C]//International conference on machine learning. PMLR, 2017: 1263-1272. 分子的监督学习在化学、药物发现和材料科学方面具有不可思议的潜力。幸运的是，文献中已经描述了几个有前途的和密切相关的神经网络模型，这些模型不受分子对称性的影响。这些模型学习消息传递算法和聚合过程来计算整个输入图的函数。在这一点上，下一步是找到这种通用方法的一个特别有效的变体，并将其应用于化学预测基准，直到我们解决它们或达到该方法的极限。在本文中，我们将现有模型重新制定为一个我们称为消息传递神经网络(MPNNs)的单一公共框架，并在该框架内探索其他新的变体。 使用mpnn，我们在一个重要的分子性质预测基准上展示了最先进的结果;这些结果足够强大，我们相信未来的工作应该集中在更大分子或更准确的地面真值标签的数据集上。 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:0:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"1 介绍 对分子结构的图监督学习已经提出了许多图神经网络模型，本文提出了MPNN框架，将这些模型都纳入这个框架当中，并探索新的变体。 提出了一种MPNN，在分子预测的13个目标上实现了最先进的结果，并在其中的11个目标上达到了化学精度 提出了几种不同的MPNN，在单独操作分子拓扑（没有空间信息作为输入）的情况下，在13个目标中的5个目标上达到了化学精度 提出了一种通用方法来训练具有更大节点表示的MPNN，而不会相应增加计算时间和内存，与以前高维节点表示的MPNN相比，节省了大量时间 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:1:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"2 MPNN MPNN在无向图$G$上操作，节点特征为$x_v$，边缘特征为$e_{vw}$，可以向有向图推广。前向传播分为两个阶段：消息传递阶段和读出阶段。消息传递阶段运行$T$步，由消息函数$M_t$和定点更新函数$U_t$定义。图中每个节点的隐藏状态$h_v^t$由消息$m_v^{t+1}$更新： $$ \\boldsymbol{m}_i^{k+1}=\\sum _{v_j\\in N(v_i)}M^{(k)}(\\boldsymbol{h}_i^{(k)},\\boldsymbol{h}j^{(k)},\\boldsymbol{e}{ij}) $$ $$ \\boldsymbol{h}_i^{(k+1)}=U_i^{(k)}(\\boldsymbol{h}_i^{(k)},\\boldsymbol{m}_i^{(k+1)}) $$ 读出函数读取整个图的特征，更新特征向量： $$ \\hat{y}=R({h_v^T|v\\in G}) $$ 其中消息函数$M_t$、定点更新函数$U_t$、读出函数$R$均为可学习的可微函数。$R$作用于节点状态集合，必须不受节点状态排列的影响。 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:2:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"3 MPNN框架理解下的已有模型 Convolutional Networks for Learning Molecular Fingerprints, Duvenaud et al. (2015) $$ M(h_v,h_w,e_{vw})=Concat(h_w,e_{vw}) $$ $$ U_t(h_v^t,m_v^{t+1})=Sigmoid(H_t^{deg(v)}m_v^{t+1}) $$ $$ R=f(\\sum_{v,t}softmax(W_th_v^t)) $$ Gated Graph Neural Networks (GG-NN), Li et al. (2016) $$ M(h_v,h_w,e_{vw})=A_{e_{vw}}h_w^t $$ $$ U_t(h_v^t,m_v^{t+1})=GRU(h_v^t,m_v^{t+1}) $$ $$ R=\\sum_{v,t}\\sigma(i(h_v^{(T)},h_v^0))\\odot(j(h_v^{(T)}) $$ Interaction Networks, Battaglia et al. (2016) $$ M(h_v,h_w,e_{vw})=Concat(h_v,h_w,e_{vw}) $$ $$ U_t(h_v^t,m_v^{t+1})=Concat(h_v,x_v,m_v) $$ $$ R=f(\\sum_{v\\in G}h_v^T) $$ Molecular Graph Convolutions, Kearnes et al. (2016) $$ M(h_v,h_w,e_{vw})=e_{vw}^t $$ $$ U_t(h_v^t,m_v^{t+1})=\\alpha(W_1(\\alpha(W_0h_v^t), m_v^{t+1}) $$ $$ e_{vw}^{t+1}=U_t’(e_{vw}^t,h_v^t,h_w^t)=\\alpha(W_4(\\alpha(W_2,e_{vw}^t),\\alpha(W_3(h_v^t,h_w^t)))) $$ 其中$\\alpha$表示ReLU函数，(…,…)表示Concat连接。 Deep Tensor Neural Networks, Schutt et al. (2017) $$ M_t=tanh(W^{fc}((W^{cf}h_w^t+b_1)\\odot(Q^{df}e_{vw}+b_2))) $$ $$ U_t(h_v^t,m_v^{t+1})=h_v^t+m_v^{t+1} $$ $$ R=\\sum_v NN(h_v^T) $$ 通过上述模型总结，可以寻找MPNN中的关键想法。 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:3:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"4 QM9数据集 数据集中的分子包括氢（H）、碳（C）、氧（O）和氮（N） ，和氟（F）原子，并且含有最多9个重（非氢）原子。总的来说，这导致了大约134k个类似药物的有机分子，这些分子跨越了广泛的化学范围。 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:4:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"5 MPNN变体 消息函数： 将分子图视为有向图，因此m向量容量变成双倍； 以GG-NN为基线作变体调整； 增加虚拟边，允许长距离传递信息 增加虚拟主节点，保存全局信息 读出函数：尝试了GG-NN的读出函数和set2set 输入表示： 化学图 距离箱 原始距离特征 ","date":"2023-05-30","objectID":"/neural-message-passing-for-quantum-chemistry/:5:0","tags":["个人笔记"],"title":"【论文简记】Neural Message Passing for Quantum Chemistry","uri":"/neural-message-passing-for-quantum-chemistry/"},{"categories":["个人笔记"],"content":"Non-local Neural Networks Wang X, Girshick R, Gupta A, et al. Non-local neural networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7794-7803. 卷积运算和循环运算都是每次处理一个局部邻域的构建块。在本文中，我们将非局部操作作为捕获远程依赖关系的通用构建块。受计算机视觉中经典的非局部均值方法[4]的启发，我们的非局部运算将一个位置的响应计算为所有位置特征的加权和。这个构建块可以插入到许多计算机视觉体系结构中。在视频分类任务上，即使没有任何花哨的东西，我们的非局部模型也可以在Kinetics和Charades数据集上竞争或优于当前的竞争获胜者。在静态图像识别中，我们的非局部模型改进了COCO任务套件上的目标检测/分割和姿态估计。代码将提供。 注意力机制的一般化总结： 在本文中，我们提出了非局部操作作为一种高效、简单和通用的组件，用于捕获深度神经网络的远程依赖关系。我们提出的非局部运算是对计算机视觉中经典的非局部均值运算的推广。直观地说，非局部操作计算的是某个位置的响应作为输入特征映射中所有位置的特征的加权和。位置集可以是空间、时间或时空，这意味着我们的操作适用于图像、序列和视频问题。 ","date":"2023-05-30","objectID":"/non-local-neural-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Non-local Neural Networks","uri":"/non-local-neural-networks/"},{"categories":["个人笔记"],"content":"NLNN $$ \\bold{y}i=\\frac{1}{\\mathcal{C}(\\bold{x})}\\sum{\\forall j}f(\\bold{x}_i,\\bold{x}_j)g(\\bold{x}_j) $$ 其中，$i$是输出位置的索引（空间，时间或者时空），$j$是所有可能位置的枚举，$\\bold{x}$是输入信号（图像，序列，影像，通常是它们的特征），$\\bold{y}$是和$\\bold{x}$相同尺寸的输出。$f$在$i$和所有$j$之间计算一个标量（表示他们之间的关系，比如联系的密切程度），$g$计算输入在$j$上的表示。输出由因子$\\mathcal{C}(\\bold{x})$标准化。 非局部操作是一个灵活的构建块，可以很容易地与卷积/循环层一起使用。它可以添加到深度神经网络的早期部分，不像最后经常使用的fc层。这允许我们构建一个结合了非本地和本地信息的更丰富的层次结构。 ","date":"2023-05-30","objectID":"/non-local-neural-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Non-local Neural Networks","uri":"/non-local-neural-networks/"},{"categories":["个人笔记"],"content":"NLNN函数的实例化 高斯函数： $$ f(\\bold{x}_i,\\bold{x}_j)=e^{\\bold{x}_i^T\\bold{x}_j} $$ 嵌入高斯函数： $$ f(\\bold{x}_i,\\bold{x}_j)=e^{\\theta (\\bold{x}i)^T\\phi(\\bold{x}j)} $$ 自注意力也是NLNN的一种特殊实例化： $$ \\bold{y}=softmax(\\bold{x}^TW\\theta^T W\\phi \\bold{x})g(\\bold{x}) $$ 点乘： $$ f(\\bold{x}_i,\\bold{x}_j)=\\theta(\\bold{x}_i)^T\\phi(\\bold{x}_j) $$ 拼接： $$ f(\\bold{x}_i,\\bold{x}_j)=ReLU(\\bold{w}_f^T[\\theta(\\bold{x}_i),\\phi(\\bold{x}_j)]) $$ 其中(…,…)表示两者的Concat。 ","date":"2023-05-30","objectID":"/non-local-neural-networks/:1:1","tags":["个人笔记"],"title":"【论文简记】Non-local Neural Networks","uri":"/non-local-neural-networks/"},{"categories":["个人笔记"],"content":"非局部块形式 $$ \\bold{z}_i=W_z\\bold{y}_i+\\bold{x}_i $$ 其中$+\\bold{x}_i$表示残差形式的连接。通过这种块构建，NLNN块可以很容易地加入到现有的网络。 ","date":"2023-05-30","objectID":"/non-local-neural-networks/:1:2","tags":["个人笔记"],"title":"【论文简记】Non-local Neural Networks","uri":"/non-local-neural-networks/"},{"categories":["个人笔记"],"content":"实验 实验比较了几种NLNN函数实现的效果，基于现有模型进行消融实验，证明得到了提升。 ","date":"2023-05-30","objectID":"/non-local-neural-networks/:1:3","tags":["个人笔记"],"title":"【论文简记】Non-local Neural Networks","uri":"/non-local-neural-networks/"},{"categories":["个人笔记"],"content":"Recurrent neural network based language model 提出了一种新的基于递归神经网络的语言模型(RNN LM)，并将其应用于语音识别。结果表明，与最先进的后退语言模型相比，使用几个RNN LMs的混合物可以减少大约50%的困惑。语音识别实验显示，在《华尔街日报》任务上，与在相同数据量上训练的模型相比，单词错误率降低了18%左右，在更难的NIST RT05任务上，错误率降低了5%左右，即使是在backoff模型比RNN LM训练的数据多得多的情况下。我们提供了充分的经验证据，表明连接主义语言模型优于标准的n-gram技术，除了它们的高计算(训练)复杂性。 Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model[C]//Interspeech. 2010, 2(3): 1045-1048. ","date":"2023-05-30","objectID":"/recurrent-neural-net-work-based-language-model/:0:0","tags":["个人笔记"],"title":"【论文简记】Recurrent neural network based language model","uri":"/recurrent-neural-net-work-based-language-model/"},{"categories":["个人笔记"],"content":"模型设计 本文提出了简单的递归神经网络，称为Elman网络架构： $$ x(t)=w(t)+s(t-1) $$ $$ s_j(t)=f(\\sum_ix_i(t)u_{ji}) $$ $$ y_k(t)=g(\\sum_js_j(t)v_{kj}) $$ 其中， $$ f(z)=\\frac{1}{1+e^{-z}} $$ $$ g(z_m)=\\frac{e^{z_m}}{\\sum_k e^{z_k}} $$ 训练分为几个epoch进行。所有训练数据都是顺序呈现的，权重初始化为小值（随机高斯噪声，平均值为零，方差为0.1）。起始学习率为$\\alpha=0.1$。每个epoch后，对网络进行验证数据测试，如果验证数据的对数似然增加，则继续进行，否则$\\alpha$减半，如果仍然没有改善，训练结束。通常训练10~20epoch后收敛。 动态训练：运行测试时仍然更新参数 请注意，统计语言建模中的训练阶段和测试阶段通常是不同的，因为模型不会在处理测试数据时得到更新。所以，如果一个新的人名在测试集中反复出现，即使它是由已知单词组成的，它也会反复得到一个非常小的概率。 可以假设，这种长期记忆不应该存在于上下文单元的激活中(因为这些单元变化非常快)，而应该存在于突触本身——即使在测试阶段，神经网络也应该继续训练。我们把这样的模型称为动态模型。对于动态模型，我们使用固定学习率α = 0.1。而在训练阶段，所有的数据都是在epoch中多次呈现给网络，而动态模型在处理测试数据时只更新一次。这当然不是最优解决方案，但正如我们将看到的，它足以获得相对于静态模型的大的困惑减少。注意，这种修改与backoff模型的缓存技术非常相似，不同之处在于神经网络是在连续空间中学习的，所以如果’ dog ‘和’ cat ‘是相关的，那么在测试数据中频繁出现’ dog ‘也会触发’ cat ‘的概率增加。 ","date":"2023-05-30","objectID":"/recurrent-neural-net-work-based-language-model/:1:0","tags":["个人笔记"],"title":"【论文简记】Recurrent neural network based language model","uri":"/recurrent-neural-net-work-based-language-model/"},{"categories":["个人笔记"],"content":"优化 为了提高性能，我们将出现频率低于阈值(在训练文本中)的所有单词合并到一个特殊的稀有标记中。单词概率计算为： $$ P(w_i(t+1)|w(t),s(t-1))= \\begin{equation} \\left{ \\begin{aligned} \\frac{y_{rare}(t)}{C_{rare}},若w_i(t+1)是rare的\\ y_i(t),其他\\ \\end{aligned} \\right. \\end{equation} $$ ","date":"2023-05-30","objectID":"/recurrent-neural-net-work-based-language-model/:2:0","tags":["个人笔记"],"title":"【论文简记】Recurrent neural network based language model","uri":"/recurrent-neural-net-work-based-language-model/"},{"categories":["个人笔记"],"content":"实验和结论 RNN 相比于 Bengio 中的 FNN 的主要优势在于没有指定固定的语境，而是使用隐藏层的状态概括之前所有的语境信息。优点包括需要指定的超参数数量少。实验发现，最朴素的RNN LM模型的效果远好于（各种） n-gram 。 ","date":"2023-05-30","objectID":"/recurrent-neural-net-work-based-language-model/:3:0","tags":["个人笔记"],"title":"【论文简记】Recurrent neural network based language model","uri":"/recurrent-neural-net-work-based-language-model/"},{"categories":["个人笔记"],"content":"Reducing the dimensionality of data with neural networks ","date":"2023-05-30","objectID":"/reducing-the-dimensionality-of-data-with-neural-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Reducing the dimensionality of data with neural networks","uri":"/reducing-the-dimensionality-of-data-with-neural-networks/"},{"categories":["个人笔记"],"content":"摘要 通过训练具有小中心层的多层神经网络重构高维输入向量，可以将高维数据转换为低维代码。梯度下降可以用于微调这种“自动编码器”网络中的权重，但只有当初始权重接近一个好的解决方案时，这种方法才有效。我们描述了一种有效的初始化权重的方法，该方法允许深度自编码器网络学习低维代码，这比主成分分析作为降低数据维数的工具要好得多。 Hinton G E, Salakhutdinov R R. Reducing the dimensionality of data with neural networks[J]. science, 2006, 313(5786): 504-507. ","date":"2023-05-30","objectID":"/reducing-the-dimensionality-of-data-with-neural-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Reducing the dimensionality of data with neural networks","uri":"/reducing-the-dimensionality-of-data-with-neural-networks/"},{"categories":["个人笔记"],"content":"内容 我们设计了一个非线性下PCA生成器（古代的神经网络），可以使用合适、多层编码网络将高维数据转为低维编码，同时使用一个相似的解码网络来从编码数据中恢复出来。 随机初始化两个网络的权重，通过最小化原始数据和重建数据的差异来同时训练两个网络。通过链式法则来反向传播导数误差得到所需要的梯度，即通过解码网络到编码网络。整个系统被称为“自编码”。 古代的BP算法具有局限性（梯度消失，没有优化器），导致初始化的结果对网络训练的结果影响很大： 如果随机初始化的值过大，通常会找到较差的局部极小值。个人想法这是因为较大的权值会本身就很接近某些极值或鞍点，会导致BP算法可以“操作”的空间减小了。 如果随机初始化的值过小，会导致前几层的梯度过小而难以训练。 我们为二进制数据引入了这种预训练过程，并将其推广到实值数据，表明它适用于各种数据集： $$ E(\\bold{v},\\bold{h})=-\\sum_{i\\in pixels}b_iv_i-\\sum_{j\\in features}b_jh_j-\\sum_{i,j}v_ih_jw_{ij} $$ 其中$v_i$和$h_j$表示二进制像素i和特征j的状态，$b_i$和$b_j$表示其偏差，$w_{ij}$表示像素和特征之间的权重。图像的训练可以通过调整权重和偏差来降低图像的能量函数，提高能量的相似性，使“虚构的”图像更加符合真实的数据。 权重的改变： $\\Delta w_{ij}=\\epsilon(\u003cv_ih_j\u003e{data}-\u003cv_ih_j\u003e{recon})$ 其中$\\epsilon$是学习率，被减数是像素i和特征检测器j的分数，后者是相应虚构的分数。 为了验证我们的预训练算法可以有效的微调我们的网络，我们使用了一个合成的曲线图数据集，改曲线图是在二维上随机选择3个自由点。对于这个数据集，真实内在的维度是已知的，像素强度和以及形成它们的非线性的6个数是相关的。像素强度在0和1之间是非高斯的，所以在自编码中我们使用逻辑输出单元，通过微调来最小化交叉熵： $$ -\\sum_ip_i\\log \\hat{p_i}-\\sum_i(1-p_i)\\log(1-\\hat{p_i}) $$ ","date":"2023-05-30","objectID":"/reducing-the-dimensionality-of-data-with-neural-networks/:2:0","tags":["个人笔记"],"title":"【论文简记】Reducing the dimensionality of data with neural networks","uri":"/reducing-the-dimensionality-of-data-with-neural-networks/"},{"categories":["个人笔记"],"content":"结论 自20世纪80年代以来，很明显，通过深度自编码器的反向传播对于非线性降维非常有效，前提是计算机足够快，数据集足够大，初始权重足够接近一个好的解决方案。现在这三个条件都满足了。与非参数方法(15,16)不同，自编码器在数据和代码空间之间给出两个方向的映射，并且它们可以应用于非常大的数据集，因为预训练和微调都随着训练案例的数量在时间和空间上呈线性增长。 ","date":"2023-05-30","objectID":"/reducing-the-dimensionality-of-data-with-neural-networks/:3:0","tags":["个人笔记"],"title":"【论文简记】Reducing the dimensionality of data with neural networks","uri":"/reducing-the-dimensionality-of-data-with-neural-networks/"},{"categories":["个人笔记"],"content":"Relational inductive biases, deep learning, and graph networks Battaglia P W, Hamrick J B, Bapst V, et al. Relational inductive biases, deep learning, and graph networks[J]. arXiv preprint arXiv:1806.01261, 2018. 人工智能(AI)最近经历了一次复兴，在视觉、语言、控制和决策等关键领域取得了重大进展。这在一定程度上是由于廉价的数据和廉价的计算资源，它们符合深度学习的天然优势。然而，人类智能的许多定义特征是在不同的压力下发展起来的，目前的方法仍然遥不可及。特别是，超越个人经验的概括——这是人类从婴儿期开始智能的标志——对现代人工智能来说仍然是一个巨大的挑战。 以下是部分意见书，部分回顾，部分统一。我们认为，组合泛化必须是人工智能实现类人能力的首要任务，而结构化表示和计算是实现这一目标的关键。就像生物学将先天和后天合作使用一样，我们拒绝在“手工工程”和“端到端”学习之间做出错误的选择，相反，我们提倡一种受益于两者优势互补的方法。我们探讨了如何在深度学习架构中使用关系归纳偏差来促进对实体、关系和构成它们的规则的学习。我们提出了一个具有强关系归纳偏差的AI工具包的新构建块-图网络-它概括和扩展了在图上操作的神经网络的各种方法，并为操作结构化知识和产生结构化行为提供了一个直接的接口。我们讨论了图网络如何支持关系推理和组合泛化，为更复杂、可解释和灵活的推理模式奠定基础。作为本文的补充，我们还发布了一个用于构建图网络的开源软件库，并演示了如何在实践中使用它们。 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:0:0","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"1 Intro 人类的智慧在于能够“无限地利用有限的手段”，从已知的构建块构建新的推断、预测和行为=\u003e组合泛化能力 人类组合泛化的能力主要取决于我们表示结构和推理关系的认知机制。我们将复杂系统表示为实体及其相互作用的组合；我们使用层次结构来抽象出细粒度的差异，并捕获表征和行为之间更普遍的共性；我们通过组合熟悉的技能和惯例来解决新问题；我们通过对齐两个领域之间的关系结构，并根据对另一个领域的相应知识对其中一个领域进行推断，从而得出类比。 世界是组成的，或者至少，我们用组成的术语来理解它。在学习时，我们要么将新知识融入现有的结构化表征中，要么调整结构本身以更好地适应(并利用)新知识和旧知识。 人工智能如何获得组合泛化的能力？ 结构化方法 端到端 近年来，深度学习和结构化方法的交叉领域出现了一类模型，他们关注于对显示的结构化数据进行推理的方法。这些方法的共同之处在于能够在离散的实体和他们之间的关系上进行计算，它们与经典方法的不同之处在于如何学习实体和关系的表示和结构以及相应的计算。值得注意的是，这些方法都带有强烈的关系归纳偏置，以特定的架构假设的形式来引导这些方法去学习实体和关系，这被认为是类人智能的一个重要组成部分。 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:1:0","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"2 关系归纳偏置 关系推理 我们将结构定义为组合一组已知构建块的产物。“结构化表示”捕获这种组合(即元素的排列)，“结构化计算”将元素及其组合作为一个整体进行操作。因此，关系推理涉及操纵实体和关系的结构化表示，并使用规则来组合它们。我们使用这些术语来捕捉认知科学、理论计算机科学和人工智能的概念，如下所示: 实体是具有属性的元素，例如具有大小和质量的物理对象。 关系是实体之间的属性。两个对象之间的关系可能包括SAME SIZE AS, HEAVIER THAN, AND DISTANCE FROM。关系也可以有属性。大于X倍的关系接受一个属性X，该属性决定了关系的相对权重阈值是真还是假。两国关系也可能对全局上下文很敏感。对于一块石头和一根羽毛来说，下降速度的关系取决于是在空气中还是在真空中。这里我们关注实体之间的成对关系。 规则是一个函数(像一个非二进制逻辑谓词)，映射实体和关系到其他实体和关系，如规模比较，如是实体X大吗?实体X是否比实体Y重?这里我们考虑接受一个或两个参数(一元和二元)并返回一元属性值的规则。 比较经典的例子是图模型，可以通过随机变量明确随机条件独立性来表示复杂的联合分布，如隐马尔可夫模型等；或者明确表示变量之间的稀疏依赖关系，从而提供各种有效的推理和推理算法，如消息传递等。 归纳偏置 归纳偏置允许学习算法优先考虑一种解决方案(或解释)而不是另一种解决方案，独立于观察到的数据。在贝叶斯模型中，归纳偏置通常通过先验分布的选择和参数化来表示。在其他情况下，归纳偏置可能是为了避免过拟合而添加的正则化项，或者它可能被编码在算法本身的架构中。归纳偏置通常以灵活性换取改进的样本复杂性，并且可以从偏置-方差权衡的角度来理解。理想情况下，归纳偏置既可以在不显著降低性能的情况下改善对解决方案的搜索，也可以帮助找到以理想方式推广的解决方案;然而，不匹配的归纳偏置也可能通过引入太强的约束而导致次优性能。 归纳偏置可以表达关于数据生成过程或解空间的假设。例如，在对数据拟合一维函数时，线性最小二乘法遵循近似函数为线性模型的约束，并且在二次惩罚下近似误差应最小。这反映了一种假设，即数据生成过程可以简单地解释为被加性高斯噪声破坏的直线过程。类似地，L2正则化优先考虑参数值较小的解，并且可以为病态问题导出唯一解和全局结构。这可以解释为一个关于学习过程的假设:当解决方案之间的模糊性较少时，寻找好的解决方案更容易。注意，这些假设不必是明确的——它们反映了对模型或算法如何与世界交互的解释。 通俗地说，归纳偏置让算法优先某种解决方案，这种偏好是独立于观测的数据的。它一般是对样本的产生过程，或者最终解的空间的一些假设。 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:2:0","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"2.1 标准深度学习构建块中的关系归纳偏置 全连接层：隐式关系归纳偏置非常弱：所有输入单元可以相互作用以确定任何输出单元的值 卷积层：局部性和平移不变性 循环层：时间不变性 集合和图：集合具有排序不变性，图是一种支持任意关系结构的表示形式 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:2:1","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"3 图网络 总结大量前人的图神经网络工作，提出 GN 框架，用于图结构表示的关系推理。GN模块包括三个更新函数和三个聚合函数： $$ \\bold{e}k’=\\phi^e(\\bold{e}k,\\bold{v}{rk},\\bold{v}{sk},\\bold{u}) \\quad \\bold{\\overline{e}}_i’=\\rho^{e\\rightarrow v}(E_i’)\\ \\bold{v}_i’=\\phi^v(\\bold{\\overline{e}}_i’,\\bold{v}_i,\\bold{u}) \\quad \\bold{\\overline{e}}’=\\rho^{e\\rightarrow u}(E’) \\ \\bold{u}_i’=\\phi^v(\\bold{\\overline{e}}’,\\bold{\\overline{v}},\\bold{u}) \\quad \\bold{\\overline{v}}’=\\rho^{v\\rightarrow u}(V’) $$ ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:3:0","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"图网络的归纳偏置 图可以表达实体之间的任意关系，这意味着GN的输入决定了表示如何相互作用和隔离 图将实体及其关系表示为集合，这些集合对置换是不变的，这意味着gn对于这些元素的顺序是不变的 GN的每边和每节点函数分别在所有边和节点上重用，这意味着GN自动支持一种形式的组合泛化 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:3:1","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"4 图网络的设计原则 灵活性：属性值的表征灵活性与图结构本身的灵活性 可配置的内部块结构 可组合的多块结构 ","date":"2023-05-30","objectID":"/relational-inductive-biases-deep-learning-and-graph-networks/:4:0","tags":["个人笔记"],"title":"【论文简记】Relational inductive biases, deep learning, and graph networks","uri":"/relational-inductive-biases-deep-learning-and-graph-networks/"},{"categories":["个人笔记"],"content":"网课：加速计算基础 —— CUDA C/C++笔记 ","date":"2023-03-31","objectID":"/cuda/:1:0","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"基础内容：线程、内存管理和异步流 1-3 01-stream-init-solution.cu #include \u003cstdio.h\u003e __global__ void initWith(float num, float *a, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { a[i] = num; } } __global__ void addVectorsInto(float *result, float *a, float *b, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { result[i] = a[i] + b[i]; } } void checkElementsAre(float target, float *vector, int N) { for(int i = 0; i \u003c N; i++) { if(vector[i] != target) { printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target); exit(1); } } printf(\"Success! All values calculated correctly.\\n\"); } int main() { int deviceId; int numberOfSMs; cudaGetDevice(\u0026deviceId); cudaDeviceGetAttribute(\u0026numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); const int N = 2\u003c\u003c24; size_t size = N * sizeof(float); float *a; float *b; float *c; cudaMallocManaged(\u0026a, size); cudaMallocManaged(\u0026b, size); cudaMallocManaged(\u0026c, size); cudaMemPrefetchAsync(a, size, deviceId); cudaMemPrefetchAsync(b, size, deviceId); cudaMemPrefetchAsync(c, size, deviceId); size_t threadsPerBlock; size_t numberOfBlocks; threadsPerBlock = 256; numberOfBlocks = 32 * numberOfSMs; cudaError_t addVectorsErr; cudaError_t asyncErr; /* * Create 3 streams to run initialize the 3 data vectors in parallel. */ cudaStream_t stream1, stream2, stream3; cudaStreamCreate(\u0026stream1); cudaStreamCreate(\u0026stream2); cudaStreamCreate(\u0026stream3); /* * Give each `initWith` launch its own non-standard stream. */ initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream1\u003e\u003e\u003e(3, a, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream2\u003e\u003e\u003e(4, b, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream3\u003e\u003e\u003e(0, c, N); addVectorsInto\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock\u003e\u003e\u003e(c, a, b, N); addVectorsErr = cudaGetLastError(); if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr)); asyncErr = cudaDeviceSynchronize(); if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr)); cudaMemPrefetchAsync(c, size, cudaCpuDeviceId); checkElementsAre(7, c, N); /* * Destroy streams when they are no longer needed. */ cudaStreamDestroy(stream1); cudaStreamDestroy(stream2); cudaStreamDestroy(stream3); cudaFree(a); cudaFree(b); cudaFree(c); } ","date":"2023-03-31","objectID":"/cuda/:1:1","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"进阶内容：2维和3维的网格和块 可以将网格和线程块定义为最多具有 3 个维度。使用多个维度定义网格和线程块绝不会对其性能造成任何影响，但这在处理具有多个维度的数据时可能非常有用，例如 2D 矩阵。如要定义二维或三维网格或线程块，可以使用 CUDA 的 dim3 类型，即如下所示： dim3 threads_per_block(16, 16, 1); dim3 number_of_blocks(16, 16, 1); someKernel\u003c\u003c\u003cnumber_of_blocks, threads_per_block\u003e\u003e\u003e(); 鉴于以上示例，someKernel 内部的变量 gridDim.x、gridDim.y、blockDim.x 和 blockDim.y 均将等于 16。 1-1 01-matrix-multiply-2d-solution.cu：2D矩阵乘法 #include \u003cstdio.h\u003e #define N 64 __global__ void matrixMulGPU( int * a, int * b, int * c ) { int val = 0; int row = blockIdx.x * blockDim.x + threadIdx.x; int col = blockIdx.y * blockDim.y + threadIdx.y; if (row \u003c N \u0026\u0026 col \u003c N) { for ( int k = 0; k \u003c N; ++k ) val += a[row * N + k] * b[k * N + col]; c[row * N + col] = val; } } void matrixMulCPU( int * a, int * b, int * c ) { int val = 0; for( int row = 0; row \u003c N; ++row ) for( int col = 0; col \u003c N; ++col ) { val = 0; for ( int k = 0; k \u003c N; ++k ) val += a[row * N + k] * b[k * N + col]; c[row * N + col] = val; } } int main() { int *a, *b, *c_cpu, *c_gpu; int size = N * N * sizeof (int); // Number of bytes of an N x N matrix // Allocate memory cudaMallocManaged (\u0026a, size); cudaMallocManaged (\u0026b, size); cudaMallocManaged (\u0026c_cpu, size); cudaMallocManaged (\u0026c_gpu, size); // Initialize memory for( int row = 0; row \u003c N; ++row ) for( int col = 0; col \u003c N; ++col ) { a[row*N + col] = row; b[row*N + col] = col+2; c_cpu[row*N + col] = 0; c_gpu[row*N + col] = 0; } dim3 threads_per_block (16, 16, 1); // A 16 x 16 block threads dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1); matrixMulGPU \u003c\u003c\u003c number_of_blocks, threads_per_block \u003e\u003e\u003e ( a, b, c_gpu ); cudaDeviceSynchronize(); // Wait for the GPU to finish before proceeding // Call the CPU version to check our work matrixMulCPU( a, b, c_cpu ); // Compare the two answers to make sure they are equal bool error = false; for( int row = 0; row \u003c N \u0026\u0026 !error; ++row ) for( int col = 0; col \u003c N \u0026\u0026 !error; ++col ) if (c_cpu[row * N + col] != c_gpu[row * N + col]) { printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col); error = true; break; } if (!error) printf(\"Success!\\n\"); // Free all our allocated memory cudaFree(a); cudaFree(b); cudaFree( c_cpu ); cudaFree( c_gpu ); } 1-1 01-heat-conduction-solution.cu：模拟金属银二维热传导的应用程序执行加速操作 #include \u003cstdio.h\u003e #include \u003cmath.h\u003e // Simple define to index into a 1D array from 2D space #define I2D(num, c, r) ((r)*(num)+(c)) __global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out) { int i00, im10, ip10, i0m1, i0p1; float d2tdx2, d2tdy2; int j = blockIdx.x * blockDim.x + threadIdx.x; int i = blockIdx.y * blockDim.y + threadIdx.y; // loop over all points in domain (except boundary) if (j \u003e 0 \u0026\u0026 i \u003e 0 \u0026\u0026 j \u003c nj-1 \u0026\u0026 i \u003c ni-1) { // find indices into linear memory // for central point and neighbours i00 = I2D(ni, i, j); im10 = I2D(ni, i-1, j); ip10 = I2D(ni, i+1, j); i0m1 = I2D(ni, i, j-1); i0p1 = I2D(ni, i, j+1); // evaluate derivatives d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10]; d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1]; // update temperatures temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2); } } void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out) { int i00, im10, ip10, i0m1, i0p1; float d2tdx2, d2tdy2; // loop over all points in domain (except boundary) for ( int j=1; j \u003c nj-1; j++ ) { for ( int i=1; i \u003c ni-1; i++ ) { // find indices into linear memory // for central point and neighbours i00 = I2D(ni, i, j); im10 = I2D(ni, i-1, j); ip10 = I2D(ni, i+1, j); i0m1 = I2D(ni, i, j-1); i0p1 = I2D(ni, i, j+1); // evaluate derivatives d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10]; d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1]; // update temperatures temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2); } } } int main() { int istep; int nstep = 200; // number of time steps // Specify our 2D dimensions const int ni = 200; const int nj = 100; float tfac = 8.418e-5; // thermal diffusivity of silver float *temp1_ref, *temp2_ref, ","date":"2023-03-31","objectID":"/cuda/:1:2","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"进阶内容：手动内存分配和复制 尽管 cudaMallocManaged 和 cudaMemPrefetchAsync 函数性能出众并能大幅简化内存迁移，但有时也有必要使用更多手动内存分配方法。这在已知只需在设备或主机上访问数据时尤其如此，并且因免于进行自动按需迁移而能够收回数据迁移成本。 此外，通过手动内存管理，您可以使用非默认流同时开展数据传输与计算工作。在本节中，您将学习一些基本的手动内存分配和拷贝技术，之后会延伸应用这些技术以同时开展数据拷贝与计算工作。 以下是一些用于手动内存管理的 CUDA 命令： cudaMalloc 命令将直接为处于活动状态的 GPU 分配内存。这可防止出现所有 GPU 分页错误，而代价是主机代码将无法访问该命令返回的指针。 cudaMallocHost 命令将直接为 CPU 分配内存。该命令可“固定”内存(pinned memory)或“页锁定”内存(page-locked memory)，此举允许将内存异步拷贝至 GPU 或从 GPU 异步拷贝至内存。固定内存过多则会干扰 CPU 性能，因此请勿无端使用该命令。释放固定内存时应使用 cudaFreeHost 命令。 无论是从主机到设备还是从设备到主机，cudaMemcpy 命令均可拷贝（而非传输）内存。 除了cudaMemcpy之外，还有cudaMemcpyAsync，只要固定了主机内存，它就可以从主机到设备或从设备到主机异步复制内存，这可以通过使用cudaMallocHost分配它来完成。 与核函数的执行类似，cudaMemcpyAsync在默认情况下仅相对于主机是异步的。默认情况下，它在默认流中执行，因此对于GPU上发生的其他CUDA操作而言，它是阻塞操作。但是，cudaMemcpyAsync函数将非默认流作为可选的第5个参数。通过向其传递非默认流，可以将内存传输与其他非默认流中发生的其他CUDA操作并发。 一种常见且有用的模式是结合使用固定主机内存，非默认流中的异步内存副本和非默认流中的核函数执行，以使内存传输与核函数的执行重叠。 01-overlap-xfer-solution.cu #include \u003cstdio.h\u003e __global__ void initWith(float num, float *a, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { a[i] = num; } } __global__ void addVectorsInto(float *result, float *a, float *b, int N) { int index = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for(int i = index; i \u003c N; i += stride) { result[i] = a[i] + b[i]; } } void checkElementsAre(float target, float *vector, int N) { for(int i = 0; i \u003c N; i++) { if(vector[i] != target) { printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target); exit(1); } } printf(\"Success! All values calculated correctly.\\n\"); } int main() { int deviceId; int numberOfSMs; cudaGetDevice(\u0026deviceId); cudaDeviceGetAttribute(\u0026numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); const int N = 2\u003c\u003c24; size_t size = N * sizeof(float); float *a; float *b; float *c; float *h_c; cudaMalloc(\u0026a, size); cudaMalloc(\u0026b, size); cudaMalloc(\u0026c, size); cudaMallocHost(\u0026h_c, size); size_t threadsPerBlock; size_t numberOfBlocks; threadsPerBlock = 256; numberOfBlocks = 32 * numberOfSMs; cudaError_t addVectorsErr; cudaError_t asyncErr; /* * Create 3 streams to run initialize the 3 data vectors in parallel. */ cudaStream_t stream1, stream2, stream3; cudaStreamCreate(\u0026stream1); cudaStreamCreate(\u0026stream2); cudaStreamCreate(\u0026stream3); /* * Give each `initWith` launch its own non-standard stream. */ initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream1\u003e\u003e\u003e(3, a, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream2\u003e\u003e\u003e(4, b, N); initWith\u003c\u003c\u003cnumberOfBlocks, threadsPerBlock, 0, stream3\u003e\u003e\u003e(0, c, N); for (int i = 0; i \u003c 4; ++i) { cudaStream_t stream; cudaStreamCreate(\u0026stream); addVectorsInto\u003c\u003c\u003cnumberOfBlocks/4, threadsPerBlock, 0, stream\u003e\u003e\u003e(\u0026c[i*N/4], \u0026a[i*N/4], \u0026b[i*N/4], N/4); cudaMemcpyAsync(\u0026h_c[i*N/4], \u0026c[i*N/4], size/4, cudaMemcpyDeviceToHost, stream); cudaStreamDestroy(stream); } addVectorsErr = cudaGetLastError(); if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr)); asyncErr = cudaDeviceSynchronize(); if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr)); checkElementsAre(7, h_c, N); /* * Destroy streams when they are no longer needed. */ cudaStreamDestroy(stream1); cudaStreamDestroy(stream2); cudaStreamDestroy(stream3); cudaFree(a); cudaFree(b); cudaFree(c); cudaFreeHost(h_c); } ","date":"2023-03-31","objectID":"/cuda/:1:3","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"PPT笔记 ","date":"2023-03-31","objectID":"/cuda/:2:0","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"编程模型 变量类型限定词： 变量声明 内存 域 生命周期 __device__ __local__ int LocalVar; 本地 线程 线程 __device__ __shared__ int SharedVar; 共享 块 块 __device__ int GlobalVar; 全局 网格 应用 __device__ __constant__ int ConstantVar; 常量 网格 应用 __local__,__shared__,__constant__的__device__可省去。 函数声明： 执行位置 调用位置 __device__ 设备 设备 __global__ 设备 主机 __host__ 主机 主机 __global__ defines a kernel function Must return void __device__ and __host__ can be used together cudaMemcpyAsync()：异步内存复制 void __syncthreads()：线程同步 时间函数： cudaEventCreate(\u0026start); cudaEventCreate(\u0026stop); cudaEventRecord(start,0); // do sth. cudaEventRecord(stop,0); cudaEventSynchronize(stop); cudaEventElapsedTime(\u0026elapsed Time, start, stop); cudaEventDestroy(start); cudaEventDestroy(stop); ","date":"2023-03-31","objectID":"/cuda/:2:1","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"内存 CUDA编程——GPU架构，由sp，sm，thread，block，grid，warp说起 CUDA学习笔记（九）内存 利用内存结构：数据切片 Typical Structure of a CUDA Program： Global variables declaration __host__ __device__… __global__, __constant__, __texture__ Function prototypes __global__ void kernelOne (…) float handyFunction (…) Main () allocate memory space on the device – cudaMalloc (\u0026d_GlblVarPtr, bytes ) transfer data from host to device cudaMemCpy (d_GlblVarPtr, h_Gl…) execution configuration setup kernel call – kernelOne«»( args… ); transfer results from device to host cudaMemCpy (h_GlblVarPtr,…) optional: compare against golden (host computed) solution Kernel – void kernelOne (type args,…) variables declaration - __local__, __shared__ automatic variables transparently assigned to registers or local memory Syncthreads ()… Other functions float handyFunction (int inVar…); ","date":"2023-03-31","objectID":"/cuda/:2:2","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"线程 warp：调度线程束的基本单位，一束线程执行相同操作。 CUDA —- Branch Divergence and Unrolling Loop ","date":"2023-03-31","objectID":"/cuda/:2:3","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"调优 通用优化策略 内存优化策略：CUDA bank 及bank conflict 指令优化策略 其他策略 ","date":"2023-03-31","objectID":"/cuda/:2:4","tags":["个人笔记"],"title":"CUDA学习笔记","uri":"/cuda/"},{"categories":["个人笔记"],"content":"并行计算基础 粒度：在并行执行过程中，二次通讯之间每个处理机计算工作量大小的一个粗略描述。分为粗粒度、细粒度。 复杂性：在不考虑通讯开销的前提下，每个处理机上的计算量最大者，即为并行计算复杂性。 并行度：算法可以并行的程度。 加速比： $$ S_p(q)=\\frac{T_s}{T_p(q)} $$ 效率： $$ E_p(q)=\\frac{S_p(q)}{q} $$ Amdahl定律：假设串行计算所需要的时间$T_s=1$，$\\alpha$是执行该计算所必须的串行部分所占的百分比，则有 $$ S_p(q)=\\frac{1}{\\alpha + \\frac{1-\\alpha}{q}} $$ Gustafson定律：假设并行计算所需要的时间$T_p=1$，$\\alpha$是执行该并行计算所需的串行部分所占的百分比，则有 $$ S_p(q)=\\frac{\\alpha+(1-\\alpha)\\times q}{1} $$ ","date":"2023-03-19","objectID":"/parallelhpc/:1:0","tags":["个人笔记"],"title":"高性能并行计算笔记","uri":"/parallelhpc/"},{"categories":["个人笔记"],"content":"矩阵乘并行计算 矩阵卷帘存储方式：对于一般$m\\times n$分块矩阵和一般的处理机阵列$p\\times q$，小块$A_{ij}$存放在处理机$P_{kl}(k=i\\mod p,l=j\\mod q)$中。 串行矩阵乘法：略，注意循环顺序。 行列分块算法： $A_i,B_i,C_{i,j},j=0,1,…,p-1$存放在$P_i$中，这种存放方式使数据在处理机中不重复。由于使用$p$个处理机，每次每个处理机计算出一个$C_{i,j}$，计算$C$需要$p$次来完成。$C_{i,j}$的计算是按对角线进行的。 其中，C按行分块，处理机间传输B的分块： 行行分块算法： $$ C_{i,*}=A_{i,*}B=\\sum_{j=0}^{p-1}A_{i,j}B_{j,*} $$ 其中，C按行分块，处理机间传送B的分块： 列行分块算法： $$ C_{*,j}=\\sum_{i=0}^{p-1}A_{*,i}B_{i,j} $$ C按列分块，处理机间通讯所传输的是计算的部分积： 列列分块算法： $$ C_{*,j}=\\sum_{i=0}^{p-1}A_{*,i}B_{i,j} $$ C按列分块，计算过程传送矩阵A的分块： Cannon算法： MPI学习笔记（四）：矩阵相乘的Cannon卡农算法 - 惊小呆520 - 博客园 (cnblogs.com) 主要的思路是A子矩阵横向循环传送，B子矩阵纵向循环传送。 子矩阵预重排： ","date":"2023-03-19","objectID":"/parallelhpc/:2:0","tags":["个人笔记"],"title":"高性能并行计算笔记","uri":"/parallelhpc/"},{"categories":["个人笔记"],"content":"LU分解 矩阵以一维卷帘方式存储，矩阵的第$i$列存放在$P_{i\\mod p}$中。 ","date":"2023-03-19","objectID":"/parallelhpc/:3:0","tags":["个人笔记"],"title":"高性能并行计算笔记","uri":"/parallelhpc/"},{"categories":["个人笔记"],"content":"1 计算机系统结构基础 冯·诺依曼结构：运算器、控制器、存储器、输入、输出=\u003e核心思想：存储程序 ","date":"2023-03-12","objectID":"/comarch/:1:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"性能、成本、功耗 性能： $$ MIPS=\\frac{Instructions}{Seconds}\\times10^{-6} $$ $$ CPUTime=\\frac{Seconds}{Program}=\\frac{Instructions}{Program}\\times\\frac{Cycles}{Instruction}\\times\\frac{Seconds}{Cycle} $$ $$ ExTime_{new}=Extime_{old}\\times(1-Fraction_{enhanced}+\\frac{Fraction_{enhanced}}{Speedup_{enhanced}}) $$ $$ Speedup_{overall}=\\frac{Extime_{old}}{ExTime_{new}} $$ 成本：略 功耗： $$ P_{total}=P_{switch}+P_{short}+P_{leakage} $$ 又 $$ P_{总}=P_{静}+P_{动}=UI $$ 时钟不翻转时，动态功耗为零，芯片相当于电阻，可求静态功耗： $$ R=\\frac{U}{I}\\ P_{静}=P_{总}-0=UI=\\frac{U^2}{R} $$ 动态功耗与时钟频率成正比。可以先求出总功耗和静态功耗，动=总-静，再利用动态功耗与时钟频率的正比关系求其他数值条件下的功耗。 ","date":"2023-03-12","objectID":"/comarch/:1:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"2 二进制与逻辑电路 ","date":"2023-03-12","objectID":"/comarch/:2:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"数的表示 定点数 原码与补码互换，都是符号位外按位取反+1 原码有+0、-0，补码的最小值1000…000没有对应的原码 原码和补码的范围： 原码 补码 $-2^{n-1}+1\\sim 2^{n-1}-1$ $-2^{n-1}\\sim 2^{n-1}-1$ 浮点数 浮点数的组成： 总位数 符号位 阶码 尾数 32 1 8 23 64 1 11 52 阶码以移码的方式表示。字长为n位，则偏移值为$2^{n-1}$。移码与补码只差一个符号位。 尾数规格化表示为1.*的形式，因小数点前面的位数肯定为1，故不存储该位，从而提升一位精度。 浮点数阶码（32位为例）的特殊情况： $0\u003ce\u003c255$，表示规格化数，此时阶码的移码表示范围为$-126\\sim 127$ $e=0$，若尾数全为零，则表示正负零；否则为非规格化数 $e=255$，若尾数为0，符号位为0或1则表示正\\负无穷大；否则为非数 ","date":"2023-03-12","objectID":"/comarch/:2:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"逻辑与电路 CMOS管： VDD是电源，PMOS管是反逻辑，开关取反；MMOS管是正逻辑，逻辑取反。$V_{in}$决定的是电路里的开关关断情况。如上为非门$\\neg A$，当输入为高电平，PMOS管取反关断，NMOS管接通，此时$V_{out}$接地，为低电平；当输入为低电平，PMOS管取反接通，NMOS管关断，此时$V_{out}$接电源，为高电平。 PMOS管在上面，NMOS管在下面。PMOS管连通电源和输出，NMOS管连通地线和输出。 观察与非情况。$\\neg A\\land B=\\neg A \\vee \\neg B$。上方PMOS管的开关取反，因此按$\\neg A \\vee \\neg B$的逻辑构建，即并联；下方NMOS管逻辑取反，按$A\\land B$的逻辑构建，即串联。 或非同理，PMOS管串联，NMOS管并联。可概括为P与并联，N与串联，P或串联，N或并联。可简单手推。 以复杂电路为例，PMOS管逻辑：$\\neg (A\\land B) \\vee (C\\land D)=(\\neg A \\vee \\neg B)\\land (\\neg C\\vee \\neg D)$，NMOS管逻辑：$\\neg\\neg (A\\land B) \\vee (C\\land D)=(A\\land B) \\vee (C\\land D)$ FO4延迟： $$ FO4延迟=本征延迟+负载延迟=翻转延迟+(延迟/电容)\\times(输入电容+连线电容)\\times4 $$ ","date":"2023-03-12","objectID":"/comarch/:2:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"3 指令系统结构 ","date":"2023-03-12","objectID":"/comarch/:3:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"MIPS一般指令选集 Inst R1, R2, R3：R2和R3 do sth.，结果放到R1 Inst R1, #num(R2)：存取到地址R2+num 对于条件判断指令，最后一个是offset（标号） ADD, ADDI, SUB, SUBI, MUL, DIV, LW, SW, BEQ（3操作数）, BNE（3操作数）, BLEZ（2操作数）, BGTZ（2操作数） SLT R1, R2, R3 ;R1=bool(R2\u003cR3) R0永远为0 MIPS汇编学习 ","date":"2023-03-12","objectID":"/comarch/:3:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"指令结构题选 指令与周期 假设在指令系统设计中考虑两种条件转移指令的设计方法： CPU A：先通过一条比较指令设置条件码A，再用一条分支指令检测条件码 CPU B：比较指令包含在分支指令中 两种CPU的条件转移指令都需要2个时钟周期，其他指令需要1个时钟周期。CPU A中全部指令的25%是条件转移指令，对应地比较指令占所有指令的25%。 设CPU A共有x个指令，可求得： $$ Cycle_A=2\\times0.25x+0.25x+0.5x=1.25x\\ Cycle_B=2\\times0.25x+0.5x=x $$ 则当 CPU A 频率为 1.2 倍时，性能是 CPU B 的 1.2/1.25=0.96 倍；当 CPU A 频率为 1.1 倍时，性能是 CPU B 的 1.1/1.25=0.88 倍。因此 CPU A 两种情况下都差。 MIPS的原子交换 LL (Load Linked) 和 SC (Store Conditional)是特殊的Load/Store指令。LL 指令的功能是从内存中读取一个字，以实现接下来的 RMW（Read-Modify-Write） 操作；SC 指令的功能是向内存中写入一个字，以完成前面的 RMW 操作。LL/SC 指令的独特之处在于，它们不是一个简单的内存读取/写入的函数，当使用 LL 指令从内存中读取一个字之后，比如 LL d, off(b)，处理器会记住 LL 指令的这次操作（会在 CPU 的寄存器中设置一个不可见的 bit 位），同时 LL 指令读取的地址 off(b) 也会保存在处理器的寄存器中。接下来的 SC 指令，比如 SC t, off(b)，会检查上次 LL 指令执行后的 RMW 操作是否是原子操作（即不存在其它对这个地址的操作），如果是原子操作，则 t 的值将会被更新至内存中，同时 t 的值也会变为1，表示操作成功；反之，如果 RMW 的操作不是原子操作（即存在其它对这个地址的访问冲突），则 t 的值不会被更新至内存中，且 t 的值也会变为0，表示操作失败。 L1: LL R1, (R3) ADDIU R2, R1, 1 SC R2, (R3) BEQ R2, 0, L1 NOP 从(R3)中读取到R1，R2=R1+1，把R2存放到(R3)，如果发现(R3)被修改过则R2被置零，用BEQ判断。 ","date":"2023-03-12","objectID":"/comarch/:3:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"4 静态流水线 五级流水线： IF(Instruction Fetch) ID(Instruction Decode) EX(Instruction Execute) MEM(Memory Access) WB(Write-Back) 取指 译码 执行 访存 写回 数据相关： 写后读（RAW）（真相关） 写后写（WAW） 读后写（WAR） 转移指令在ID执行，减少等待拍数。如转移指令需要的数据未计算得出，则它会在ID堵塞 前递/旁路：前面的运算输出直接作为后面指令的输入，是特殊结构 指令重排：使相关隔得足够远，如存取和计算分开。 分支预测：在流水线时空图上表示为无视转移指令继续执行，如果预测错误则下一拍重来 ","date":"2023-03-12","objectID":"/comarch/:4:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"ALU Verilog 书中简单CPU指令含义： 名称 含义 ADDU 无符号加 SUBU 无符号减 SLT 左小右大为真 SLTU 无符号左小右大为真 AND 与 OR 或 XOR 异或 NOR 或非 SLLV 逻辑左移 SRLV 逻辑右移 SRAV 算数右移 module alu_module ( input [ 3:0] aluop, // 4位宽的alu操作符 input [31:0] in1, // 32位宽的输入数1 input [31:0] in2, // 32位宽的输入数2 output [31:0] out // 32位宽的输出数 ); wire slt_res; // slt为比较的意思，在汇编中，若前者小于后者则为1，否则为0 // 用或连接，以下任一条件成立即为1 // 这段代码用于比较有符号数的大小，讨论四种情况：前负后正，前负后负，前正后正，前正后负 assign slt_res = (in1[31] \u0026\u0026 !in2[31]) \u0026 1’b1 | // in1[31]为1且in2[31]为0为真时 (in1[31] \u0026\u0026 in2[31] ) \u0026 !(in1\u003cin2) | // in1[31]为1且in2[31]为1且in1\u003e=in2时 (!in1[31] \u0026\u0026 !in2[31]) \u0026 (in1\u003cin2) | // in1[31]为0且in2[31]为0且in1\u003cin2时 (!in1[31] \u0026\u0026 in2[31]) \u0026 1’b0; // in1[31]为0且in2[31]为1为假时 // 操作码和书上有些对不上，理解意思即可。 // slt逻辑需要手写，注意区分有符号和无符号的做法。 assign out = {32{aluop==4’b0000}} \u0026 (in1+in2) | // ADDU {32{aluop==4’b0001}} \u0026 (in1-in2) | // SUBU {32{aluop==4’b0010}} \u0026 {31’b0, slt_res} | // SLT {32{aluop==4’b0011}} \u0026 {31’b0, in1\u003cin2} | // \u003c {32{aluop==4’b0100}} \u0026 (in1\u0026in2) | // \u0026 {32{aluop==4’b0101}} \u0026 (in1|in2) | // | {32{aluop==4’b0110}} \u0026 (in1^in2) | // ^ {32{aluop==4’b0111}} \u0026 ~(in1|in2) | // ~^ {32{aluop==4’b1000}} \u0026 (in1\u003c\u003cin2) | // \u003c\u003c (逻辑移位) {32{aluop==4’b1010}} \u0026 (in1\u003e\u003ein2) | // \u003e\u003e {32{aluop==4’b1011}} \u0026 (in1\u003e\u003e\u003ein2); // verilog2001 中算术右移的新表达方式 endmodule ","date":"2023-03-12","objectID":"/comarch/:4:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"5 动态流水线 ","date":"2023-03-12","objectID":"/comarch/:5:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"静态调度 循环展开： 一个循环展开多做几次，通过修改每次的偏移量实现 寄存器重命名消除假相关 指令重排序 软流水： 软流水的方法–国科大体系结构 期末必考题 ","date":"2023-03-12","objectID":"/comarch/:5:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"动态调度 Tomasulo： 通过硬件寄存器重命名消除WAR和WAW相关，并乱序执行 Tomasulo过程： 发射：把操作队列的指令根据操作类型送到保留站（如果保留站 有空），发射过程中读寄存器的值和结果状态域 执行：如果所需的操作数都准备好，则执行，否则侦听结果总线 并接收结果总线的值。 写回：把结果送到结果总线，释放保留站 指令在发射的时候会更新寄存器状态表，如果后序指令和前序指令的目的寄存器重合了，就用后序指令的写信息标志寄存器，表示只会把后序指令的计算结果写进寄存器，这样可以解决WAW 重排序缓存ROB（ReOrder Buffer） 为实现精确例外，把后面指令对机器状态的修改延迟到前面指令已经执行完 寄存器和保留站记录的标号改成ROB，每次发射一条指令都FIFO在ROB占坑 会依次写回寄存器，但是寄存器的标号仍然是WAW中的最后一次写入的标号 ","date":"2023-03-12","objectID":"/comarch/:5:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"多发射数据通路 ","date":"2023-03-12","objectID":"/comarch/:6:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"保留站的组织（指令缓存的结构） 独立保留站、分组保留站、全局保留站 ","date":"2023-03-12","objectID":"/comarch/:6:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"寄存器与保留站的关系（读取寄存器内容的时间） 保留站前读寄存器（Tomasulo算法）： 寄存器的输出作为保留站的输入 操作数没有准备好就读寄存器，保留站侦听结果总线获取没写回的值 有序读寄存器 保留站中值的来源：寄存器、重命名寄存器、进入保留站时侦听、进入保留站后侦听 保留站中有值域，因此较复杂 寄存器读端口数为发射宽度 保留站后读寄存器： 保留站的输出作为寄存器的输入 保留站确信所有值都已经准备好后再读寄存器，有可能有Forward的情况 乱序读寄存器 保留站中没有值域，比较简单 寄存器读端口数为相应功能部件数 ","date":"2023-03-12","objectID":"/comarch/:6:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"寄存器重命名方法 重命名的方法不拘一格。 硬件重命名的分类： 重命名寄存器和结构寄存器分开 如重命名到保留站、ROB、专门的重命名寄存器、发射队列 重命名寄存器和结构寄存器不分开 需要映射表，可以有逻辑寄存器那么多项，也可以有物理寄存器那么多项 硬件重命名方法概括 一种是独立的重命名寄存器，把运算结果写回到独立的重命名寄存器，提交的时候再从重命名寄存器写到结构寄存器，指令被取消时确认结构寄存器的内容为最新内容并清空重命名寄存器 一种使用物理寄存器堆，并通过一个映射表建立逻辑寄存器和物理寄存器的映射关系，给每条指令的目标寄存器分配物理寄存器，指令提交时确认该物理寄存器为结构寄存器并释放老的物理寄存器，指令取消的时候只修改重命名表以取消后面已经建立的重命名关系 ","date":"2023-03-12","objectID":"/comarch/:6:3","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"乱序数据通路总结 三个因素两两搭配，共有$3\\times 2\\times2=12$个组合： 单项、分组、全局保留站？ 保留站前/后读寄存器？ 独立重命名寄存器/物理寄存器堆？ ","date":"2023-03-12","objectID":"/comarch/:6:4","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"7 转移预测 ","date":"2023-03-12","objectID":"/comarch/:7:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"软件方法解决控制相关 循环展开、软流水 ","date":"2023-03-12","objectID":"/comarch/:7:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"硬件转移预测技术 转移模式历史表PHT（Pattern History Table） 每项都是两位饱和计算器，T+1，N-1，高位为1预测T，高位为0预测N 转移历史寄存器BHR（Branch History Register） 考虑一种情况，分支TNTNTNTN，这样两位饱和计算器准确率可能为0 BHR是一个记录转移历史的移位寄存器，根据BHR的值，选择不同的PHT项 如，BHR为NT，选择预测N的PHT；BHR为TN，选择预测T的PHT 从而捕捉到了TNTNTN的变化规律，PHT不会反复横跳，提升预测准确率 PHT+BHR 类似Cache的全相联组相联直接相联，BHR也有3种PC索引的情况：完整PC索引（1对1）、低位PC索引，全局共用 PHT索引也有3种方式：只用BHR（g）、用PC和BHR（p）、用部分PC和BHR（s） Global PHT Per-address PHTs per-Set PHTs Global BHR GAg GAp GAs Per-address BHR PAg PAp PAs per-Set BHR SAg SAp SAs ","date":"2023-03-12","objectID":"/comarch/:7:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"8 功能部件 ","date":"2023-03-12","objectID":"/comarch/:8:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"先行进位加法器 module add16(a, b, cin, out, cout); input [15:0] a; input [15:0] b; input cin; // 低位进位 output [15:0] out; // 计算结果 output cout; // 高位进位 wire [15:0] p = a|b; // 进位传递因子，两个或者为1可能进位，即表示可以传递低位进位 wire [15:0] g = a\u0026b; // 进位生成因子，两个为1必定进位 wire [3:0] P, G; wire [15:0] c; // 0, 4, 8, 12是低位进位，其他是待求解的高位进位 assign c[0] = cin; C4 C0_3(.p(p[3:0]),.g(g[3:0]),.cin(c[0]),.P(P[0]),.G(G[0]),.cout(c[3:1])); // 注意调用的写法 C4 C4_7(.p(p[7:4]),.g(g[7:4]),.cin(c[4]),.P(P[1]),.G(G[1]),.cout(c[7:5])); C4 C8_11(.p(p[11:8]),.g(g[11:8]),.cin(c[8]),.P(P[2]),.G(G[2]),.cout(c[11:9])); C4 C12_15(.p(p[15:12]),.g(g[15:12]),.cin(c[12]),.P(P[3]),.G(G[3]),.cout(c[15:13])); // 前4个分块都算好以后，再合起来做一个C4 C4 C_INTER(.p(P),.G(G),.cin(c[0]),.P(),.G(),.cout({c[12],c[8],c[4]})); // 大括号括起来做一个整体 assign cout = (a[15]\u0026b[15]) | (a[15]\u0026c[15]) | (b[15]\u0026c[15]); // 三个中有俩是真的 assign out = (~a\u0026~b\u0026c)|(~a\u0026b\u0026~c)|(a\u0026~b\u0026~c)|(a\u0026b\u0026c); // 三个中至少有一个是真的，该位就可以为1 endmodule /* 4位并行进位块 */ module C4(p,g,cin,P,G,cout) input [3:0] p, g; // 进位传递因子和进位生成因子 input cin; // 低位进位 output P,G; // 传递给下一个块的进位生成因子和进位传递因子 output [2:0] cout; assign cout[0]=g[0]|(p[0]\u0026cin); // c_1 assign cout[1]=g[1]|(p[1]\u0026g[0])|(p[1]\u0026p[0]\u0026cin); // c_2 assign cout[2]=g[2]|(p[2]\u0026g[1])|(p[2]\u0026p[1]\u0026g[0])|(p[2]\u0026p[1]\u0026p[0]\u0026cin); // c_3 assign G=g[3]|(p[3]\u0026g[2])|(p[3]\u0026p[2]\u0026g[1])|(p[3]\u0026p[2]\u0026p[1]\u0026g[0]); // c_4，有进位生成 assign P=\u0026p; // 如果全部为1，则表示该块可以传递低位进位 endmodule ","date":"2023-03-12","objectID":"/comarch/:8:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"定点补码乘法器 Booth 2位乘 $$ [X]_补\\times [Y]_补=[X]_补\\times\\{[Y]_补的最高位取反，其他位不变的数\\} $$ $$ (-y_{31}\\times 2^{31}+y_{30}\\times 2^{30}+…+y_1\\times 2^1+y_0\\times 2^0) \\\\ = (y_{29}+y_{30}-2\\times y_{31})\\times 2^{30}+(y_{27}+y_{28}-2\\times y_{29})\\times 2^{28}+… \\\\ +(y_1+y_2-2\\times y_3)\\times 2^2 + (y_{-1}+y_0-2\\times y_1)\\times 2^0 $$ 其中$y_{-1}$为0。因此可以每次移2位，通过括号内的计算结果求该次移位求和是以下操作之一：$-2[X]_补,-[X]_补, 0,+[X]_补,+2[X]_补$。 华莱士树 堆叠加法器。 ","date":"2023-03-12","objectID":"/comarch/:8:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"9 高速缓存 ","date":"2023-03-12","objectID":"/comarch/:9:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"页着色 Cache相联 Cache相联包括全相联、直接相联、组相联。全相联指任意页可以映射到任意内存块，直接相联指特定的页只能映射到特定内存块，组相联指特定的页可以映射到某一组内存块中的任意一个。直接相联其实也是“一路组相联”。 组的编号是由地址的高位决定的。 虚-实与Index-Tag 地址分为虚地址和实地址。虚地址是逻辑地址，实地址是物理地址。页上记录的是虚地址，因此映射到Cache的时候需要转换为内存的实地址。Index是页到Cache的映射，Tag是Cache到内存的映射。 考虑各种因素，采用VIPT（虚索引，实标记）的方式进行转换和映射。页映射到Cache时兵分两路，一边直接用虚索引与Cache匹配，另一边传送到TLB，由TLB转换为实标记再匹配。 页和对应Cache块的低位一致，因此虚索引匹配就是低位匹配。低位不包括cache line的块内偏移位。$VI_{len}=log_2(C_{cache}\\div C_{cache\\;line}\\div n_{set})$，其中C指容量。 页地址在被转换为物理Tag后，会映射到对应的Cache line上。虚实地址的低位是相同的。如果页地址低位的长度比VI长，那么尽管这些页会被映射到同一个Cache line中，但他们其实对应的是不同的物理地址。就产生了一个虚地址对应多个实地址的别名问题。为了解决别名问题，我们采用页着色的方法，强制指定这些页映射到不同的Cache line中。页着色标号从cache地址的高位开始标号。 页着色例子 例如，现有4KB页大小，L1 cache大小为32KB，四路组相联，cache line大小为32B。对于该页对应的内存，虚实地址的[11:0]12位数都是相同的；cache地址为[14:0]。cache line对应[4:0]5位。此时VI长度为$log_2(32KB\\div 32B \\div 4)=8$，即[12:5]8位。这时，可能有两个[11:0]相同，但第13、14位不同的页映射到同一个cache line中，因此需要2位页着色。页着色位为第13、14位。 ","date":"2023-03-12","objectID":"/comarch/:9:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"不命中的损耗 $$ MissPenalty_{L1}=HitTime_{L2} + MissPenalty_{L2} \\times MissRate_{L2} $$ ","date":"2023-03-12","objectID":"/comarch/:9:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"10 存储管理 ","date":"2023-03-12","objectID":"/comarch/:10:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"TLB例外 MIPS中的三种TLB例外： TLB重填例外：TLB中没有相应项 TLB失效例外：相应的物理页不在内存中（包括非法存，非法取） TLB修改例外：非法写只读页 TLB例外例题：在一台Linux（页大小为4KB） 系统的MIPS计算机执行如下程序，请问发生了多少次例外？ void cycle(double *a) { int i; double b[65536]; for(i = 0; i \u003c 3; i++) { memcpy(a, b, sizeof(b)); } } 答：假设操作系统页大小为4KB，同时系统的物理内存足够大，即页表分配的物理内存在程序执行过程中不会被交换到swap区上。并且在后续的分析中忽略代码和局部变量i所在的页的影响。 a、b两数组大小均为65536×8=512KB。再假设a、b两数组的起始地址恰为一页的起始地址。 那么a、b两数组各自均需要512KB/4KB=128个页表项。 所以a、b的访问造成的TLB invalid 例外次数为128+128=256次； 假设TLB表项为128项，每项映射连续的偶数奇数两个页，采用LRU算法。那么第一次循环 中，a、b两数组每访问相邻偶数奇数两页的首地址时，均会触发一次TLB refill例外。后续各次循环TLB中均命中。那么共触发的TLB refill例外次数为128/2 + 128/2 = 128次。 ","date":"2023-03-12","objectID":"/comarch/:10:1","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"TLB Verilog 已知一台计算机的虚地址为 48 位，物理地址为 40 位，页大小为 16KB，TLB 为 64 项全相联，TLB 的每项包括一个虚页号 vpn，一个物理页号 pfn，以及一个有效位 valid，请根据如下模块接口写出一个 TLB 的地址查找部分的 Verilog 代码。 module tlb_cam(vpn_in, pfn_out, hit,…);其中vpn_in为输入的虚页号，pfn_out为输出的物理页号，hit为表示是否找到的输出信号，…表示与该 TLB 输入输出有关的其他信号。重复的代码可以用…来简化。 module tlb_cam(vpn_in, pfn_out, hit, valid_out); input [33:0] vpn_in; // 输入的虚页号 output [25:0] pfn_out; // 输出的物理页号 output hit; // 是否找到 output valid_out; reg[60:0] cam_content [63:0];//[60:60] valid [59:34] pfn [33:0] vpn; 前面是位宽，后面是数组容量 wire [63:0] entry_hit; // 逐个比较 assign entry_hit[0] = (vpn_in==cam_content[0][33:0]); assign entry_hit[1] = (vpn_in==cam_content[1][33:0]); … assign entry_hit[63] = (vpn_in==cam_content[63][33:0]); assign hit = |entry_hit; // entry_hit相当于mask，命中则\u0026通过 assign pfn_out = {26{entry_hit[ 0]}} \u0026 cam_content[0][59:34] | {26{entry_hit[ 1]}} \u0026 cam_content[1][59:34] | … {26{entry_hit[63]}} \u0026 cam_content[63][59:34]; // 有效位valid_out=命中\u0026\u0026该位有效位 assign valid_out = entry_hit[ 0] \u0026\u0026 cam_content[ 0][60] || entry_hit[ 1] \u0026\u0026 cam_content[ 1][60] || … entry_hit[63] \u0026\u0026 cam_content[63][60]; endmodule ","date":"2023-03-12","objectID":"/comarch/:10:2","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"11 多处理器系统 基于目录的cache一致性协议：为每一存储行维持一个目录项，该目录项记录所有当前持有该行备份的处理器号以及此行是否已被改写等信息。当一个处理器欲往某存储行写数且可能引起数据不一致时，它就根据目录的内容只向持有该行的备份的那些处理器发出写使无效/写更新信号，从而避免广播。 问：在基于目录的cache一致性系统中，目录记载了P1处理器已经有数据块A的备份。 哪些情况下目录会又收到一个P1对A块访问的请求？ 如何处理上述情况？ 答： 可能 P1 已经把 A 替换出去了，随后 P1 又希望访问 A，因此向目录发出了 A 的 miss 请求。如果网络不能保证 P1 发出的替换 A 消息和 miss 访问 A 消息达到目录的顺序，后发出的A 的 miss 请求越过先发出的 A 的替换请求，先到达目录，就会产生上述现象。 两种可行的处理方式： 网络保证点到点消息的顺序性； 目录发现不一致时，暂缓 miss 请求的处理，等待替换消息到达后，目录状态正确后，再返回 miss 请求的响应。 ","date":"2023-03-12","objectID":"/comarch/:11:0","tags":["个人笔记"],"title":"计算机体系结构复习笔记","uri":"/comarch/"},{"categories":["个人笔记"],"content":"在阅读本文之前，你应当已理解了Cooley-Tukey FFT算法： 关于FFT运算的理解与入门，推荐观看该视频：快速傅里叶变换(FFT)——有史以来最巧妙的算法？ ； 关于FFT的进一步理解以及蝶形运算的理解，推荐阅读《算法导论》第30章； 关于库利-图基算法的进一步理解可参考网上各种博客。 关于Stockham FFT算法： 关于Stockham FFT的比较数学的介绍，推荐该文章：原地且自动整序的FFT算法 本文参考文档：OTFFT文档中的Stockham FFT介绍 库利-图基算法是经典的FFT算法，然而会导致得到的FFT序列顺序改变，因此需要位逆序的方法排布子FFT的顺序。Stockham FFT在变换的过程中调整顺序。这种变换不能通过原地操作实现，通过将计算的中间结果存储到另一片区域，下次变换的时候再存储回来，如此往复即为Stockham FFT算法（图片来自网络）： 下面参考OTFFT的介绍和代码，从Cooley-Tukey FFT开始，讨论如何一步步得到Stockham FFT。 ","date":"2023-03-02","objectID":"/stockhamfft/:0:0","tags":["个人笔记"],"title":"从Cooley-Tukey FFT到Stockham FFT","uri":"/stockhamfft/"},{"categories":["个人笔记"],"content":"1 Cooley-Tukey FFT算法及变形 原始Cooley-Tukey FFT算法代码如下所示： #include \u003ccomplex\u003e #include \u003ccmath\u003e #include \u003cutility\u003e typedef std::complex\u003cdouble\u003e complex_t; void butterfly(int n, int q, complex_t* x) // Butterfly operation // n : sequence length // q : block start point // x : input/output squence { const int m = n/2; const double theta0 = 2*M_PI/n; if (n \u003e 1) { for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + p + 0]; const complex_t b = x[q + p + m]; x[q + p + 0] = a + b; x[q + p + m] = (a - b) * wp; } butterfly(n/2, q + 0, x); butterfly(n/2, q + m, x); } } void bit_reverse(int n, complex_t* x) // Bitreversal operation // n : squence length // x : input/output sequence { for (int i = 0, j = 1; j \u003c n-1; j++) { for (int k = n \u003e\u003e 1; k \u003e (i ^= k); k \u003e\u003e= 1); if (i \u003c j) std::swap(x[i], x[j]); } } void fft(int n, complex_t* x) // Fourier transform // n : sequence length // x : input/output sequence { butterfly(n, 0, x); bit_reverse(n, x); for (int k = 0; k \u003c n; k++) x[k] /= n; } void ifft(int n, complex_t* x) // Inverse Fourier transform // n : sequence length // x : input/output sequence { for (int p = 0; p \u003c n; p++) x[p] = conj(x[p]); butterfly(n, 0, x); bit_reverse(n, x); for (int k = 0; k \u003c n; k++) x[k] = conj(x[k]); } 上述算法需要bit_reverse()重排顺序。Stockham FFT的目标是去掉重排序这一步。为了得到Stockham FFT算法，让我们先把Cooley-Tukey FFT改写成不需要调用独立的bit_reverse()函数的形式： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft0(int n, int q, complex_t* x, complex_t* y) // n : sequence length // q : block start point // x : input/output sequence // y : work area { const int m = n/2; const double theta0 = 2*M_PI/n; if (n \u003e 1) { for (int p = 0; p \u003c m; p++) { // Butterfly operation const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + p + 0]; const complex_t b = x[q + p + m]; y[q + p + 0] = a + b; y[q + p + m] = (a - b) * wp; } fft0(n/2, q + 0, y, x); fft0(n/2, q + m, y, x); for (int p = 0; p \u003c m; p++) { // Composition of even components and odd components x[q + 2*p + 0] = y[q + p + 0]; // Even components x[q + 2*p + 1] = y[q + p + m]; // Odd components } } } void fft(int n, complex_t* x) // Fourier transform // n : sequence length // x : input/output sequence { complex_t* y = new complex_t[n]; // Allocation of work arera fft0(n, 0, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] /= n; } void ifft(int n, complex_t* x) // Inverse Fourier transform // n : sequence length // x : input/output sequence { for (int p = 0; p \u003c n; p++) x[p] = conj(x[p]); complex_t* y = new complex_t[n]; // Allocation of work area fft0(n, 0, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] = conj(x[k]); } 这样改写过后，蝶形运算和重排序被合并到一个函数处理，fft()也被调整为入口函数fft()和fft0()。在fft0()中，通过x[], y[]两个数组来回存放，在完成蝶形运算后再把重排序的结果放到一个数组中去的办法实现了重排序。 这种做法可能比原始的Cooley-Tukey FFT更慢，但这是逐步得到Stockham FFT的第一步。 ","date":"2023-03-02","objectID":"/stockhamfft/:1:0","tags":["个人笔记"],"title":"从Cooley-Tukey FFT到Stockham FFT","uri":"/stockhamfft/"},{"categories":["个人笔记"],"content":"2 递归版本的Stockham FFT 分析上面的代码，可以发现有两个位置要对数组访问，一个是蝶形运算，另一个是奇数下标和偶数下标的重新组合。如果我们能同时执行这两个操作，那么多余的访存操作就可以被消除。通过这种改动，我们就得到了递归版本的Stockham FFT： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft1(int n, int s, int q, complex_t* x, complex_t* y); void fft0(int n, int s, int q, complex_t* x, complex_t* y) // n : sequence length // s : stride // q : selection of even or odd // x : input/output sequence // y : work area { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) {} else { for (int p = 0; p \u003c m; p++) { // Butterfly operation and composition of even components and odd components const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } fft1(n/2, 2*s, q + 0, y, x); // Even place FFT (y:input, x:output) fft1(n/2, 2*s, q + s, y, x); // Odd place FFT (y:input, x:output) } } void fft1(int n, int s, int q, complex_t* x, complex_t* y) // n : sequence length // s : stride // q : selection of even or odd // x : input sequence // y : output sequence { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) { y[q] = x[q]; } else { for (int p = 0; p \u003c m; p++) { // Butterfly Operation and composition of even components and odd components const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } fft0(n/2, 2*s, q + 0, y, x); // Even place FFT (y:input/output, x:work area) fft0(n/2, 2*s, q + s, y, x); // Odd place FFT (y:input/output, x:work area) } } void fft(int n, complex_t* x) // Fourier transform // n : sequence length // x : input/output sequence { complex_t* y = new complex_t[n]; // Allocation of work area fft0(n, 1, 0, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] /= n; } void ifft(int n, complex_t* x) // Inverse Fourier transform // n : sequence length // x : input/output sequence { for (int p = 0; p \u003c n; p++) x[p] = conj(x[p]); complex_t* y = new complex_t[n]; // Allocation of work area fft0(n, 1, 0, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] = conj(x[k]); } 观察这段代码，有两个特点： 利用两块内存空间，在蝶形操作的时候将x[q + s*(p + 0)], x[q + s*(p + m)]的相互运算结果存放到y[q + s*(2*p + 0)], y[q + s*(2*p + 1)]中，从而合并了蝶形运算和重排序操作； 是互相调用的递归关系。关于这个代码设计，原文的解释是这样： “This code has become a mutual recursion in order to minimize the accesses to an array. Stockham algorithm requires work area y for sorting. It saves sorted results once in the work area. If the saved results are written back immediately to x, the program is able to avoid becoming a mutual recursion. But, if we do so, it becomes meaningless that we have reduced the accesses to an array by executing butterfly operation and composition at the same time. For this reason, this code has become a mutual recursion.” 百度翻译：为了最小化对数组的访问，这段代码变成了相互递归。Stockham算法需要工作区y进行排序。它将排序结果保存在工作区中一次。如果保存的结果立即写回x，则程序能够避免成为相互递归。但是，如果我们这样做，那么通过同时执行蝶形运算和组合来减少对数组的访问就变得毫无意义了。由于这个原因，这段代码变成了相互递归。 我暂时还没有搞懂。 从而，我们可以得到Stockham FFT的数学推导式，其中$n=2^{L-h},~m=2^{-1}n,~s=2^h,~x_h(q,p)=x_h[q+sp]$： $$ x_{h+1}(q,p)=x_h(q,p)+x_h(q,p+m) \\\\ x_{h+1}(q+s,p)=(x_h(q,p)-x_h(q,p+m))W_N^{sp} \\\\ q=0,1,…,s-1 \\\\ p=0,1,…,m-1 $$ $x_h[]$是第h步计算。当从$x_0[]$开始，得到$x_L[]$，FFT计算就完成了。 上述程序是基于频域抽取的FFT算法（DIF-FFT），基于时域抽取的FFT算法（DIT-FFT）如下所示： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft1(int n, int s, int q, complex_t* x, complex_t* y); void fft0(int n, int s, int q, complex_t* x, complex_t* y) // n : sequence length // s : stride // q : selection of even or odd // x : input/output sequence // y : work area { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) {} else { fft1(n/2, 2*s, q + 0, y, x); // Even place FFT(x:input, y:output) fft1(n/2, 2*s, q + s, y, x); // Odd place FFT(x:input, y:output) for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const co","date":"2023-03-02","objectID":"/stockhamfft/:2:0","tags":["个人笔记"],"title":"从Cooley-Tukey FFT到Stockham FFT","uri":"/stockhamfft/"},{"categories":["个人笔记"],"content":"3 Stockham FFT的迭代优化 观察上个Stockham FFT的版本，变量q可改写为循环迭代的形式： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft1(int n, int s, complex_t* x, complex_t* y); void fft0(int n, int s, complex_t* x, complex_t* y) // n : sequence length // s : stride // x : input/output sequence // y : work area { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) {} else { for (int q = 0; q \u003c s; q++) { // Iteration for recursive-call for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } } fft1(n/2, 2*s, y, x); // The number of recursive-calls is one because we tie calls with an iteration. } } void fft1(int n, int s, complex_t* x, complex_t* y) // n : sequence length // s : stride // x : input sequence // y : output sequence { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) { for (int q = 0; q \u003c s; q++) y[q] = x[q]; } else { for (int q = 0; q \u003c s; q++) { // Iteration for recursive-call for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } } fft0(n/2, 2*s, y, x); // The number of recursive-calls is one because we tie calls with an iteration. } } void fft(int n, complex_t* x) // Fourier transform // n : sequence length // x : input/output sequence { complex_t* y = new complex_t[n]; // Allocation of work area fft0(n, 1, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] /= n; } void ifft(int n, complex_t* x) // Inverse Fourier transform // n : sequence length // x : input/output sequence { for (int p = 0; p \u003c n; p++) x[p] = conj(x[p]); complex_t* y = new complex_t[n]; // Allocation of work area fft0(n, 1, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] = conj(x[k]); } 当递归的深度变深时，步长s会变大，导致访存操作的跳跃比较大，因此可以将p和q的循环对调： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft1(int n, int s, complex_t* x, complex_t* y); void fft0(int n, int s, complex_t* x, complex_t* y) // n : sequence length // s : stride // x : input/output sequence // y : work area { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) {} else { for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); for (int q = 0; q \u003c s; q++) { const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } } fft1(n/2, 2*s, y, x); } } void fft1(int n, int s, complex_t* x, complex_t* y) // n : sequence length // s : stride // x : input sequence // y : output sequence { const int m = n/2; const double theta0 = 2*M_PI/n; if (n == 1) { for (int q = 0; q \u003c s; q++) y[q] = x[q]; } else { for (int p = 0; p \u003c m; p++) { const complex_t wp = complex_t(cos(p*theta0), -sin(p*theta0)); for (int q = 0; q \u003c s; q++) { const complex_t a = x[q + s*(p + 0)]; const complex_t b = x[q + s*(p + m)]; y[q + s*(2*p + 0)] = a + b; y[q + s*(2*p + 1)] = (a - b) * wp; } } fft0(n/2, 2*s, y, x); } } void fft(int n, complex_t* x) // Fourier transform // n : sequence length // x : input/output sequence { complex_t* y = new complex_t[n]; fft0(n, 1, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] /= n; } void ifft(int n, complex_t* x) // Inverse Fourier transform // n : sequence length // x : input/output sequence { for (int p = 0; p \u003c n; p++) x[p] = conj(x[p]); complex_t* y = new complex_t[n]; fft0(n, 1, x, y); delete[] y; for (int k = 0; k \u003c n; k++) x[k] = conj(x[k]); } 最后，去除掉代码的重复部分： #include \u003ccomplex\u003e #include \u003ccmath\u003e typedef std::complex\u003cdouble\u003e complex_t; void fft0(int n, int s, bool eo, complex_t* x, complex_t* y) // n : sequence length // s : stride // eo : x is output if eo == 0, y is output if eo == 1 // x : input sequence(or output sequence if eo == 0) // y : work ","date":"2023-03-02","objectID":"/stockhamfft/:3:0","tags":["个人笔记"],"title":"从Cooley-Tukey FFT到Stockham FFT","uri":"/stockhamfft/"},{"categories":["个人笔记"],"content":"1 食谱问题（Diet Problem） 几种食物的每单位所含营养物质及其价格如下表所示，若每天需要2000千卡能量、55克蛋白质和800毫克钙，请问如何构造一个食物的组合，使得满足每天的营养物质需求且耗费金钱最少。 Oatmeal Whole milk Cherry pie Pork with beans 价格 3 9 20 19 能量 110 160 420 260 蛋白质 4 8 4 14 钙 2 285 22 80 解析： 线性规划问题的最基本形式。可设购买每种食物$x_1,x_2,x_3,x_4$单位，可得线性规划方程： $$ min\\; z=3x_1 + 9x_2 + 20x_3 + 19x_4 \\\\ \\begin{align*} s.t.\\; 110x_1 + 160x_2 + 420x_3 + 260x_4 \u0026≥ 2000 \\\\ 4x_1 + 8x_2 + 4x_3 + 14x_4 \u0026≥ 55 \\\\ 2x_1 + 285x_2 + 22x_3 + 80x_4 \u0026≥ 800 \\\\ x_1 , x_2 , x_3 , x_4 \u0026≥ 0 \\end{align*} $$ 上式可简写为： $$ min\\; z=c^Tx \\\\ \\begin{align*} s.t.\\; Ax\u0026=b \\\\ x\u0026\\geq 0 \\end{align*} $$ ","date":"2023-01-10","objectID":"/cs-lp/:1:0","tags":["个人笔记"],"title":"算法问题的线性规划建模","uri":"/cs-lp/"},{"categories":["个人笔记"],"content":"2 最大流问题（Maximum Flow Problem） 输入： 一个有向图$G\u003cV,E\u003e$，每条边$e=(u,v)$与容量$C(u,v)$相对应，源节点为$s$，目标节点为$t$。 输出： 求解每条边$e=(u,v)$对应的流量$0\\leq f(u,v)\\leq C(u,v)$，使到达目标节点的总流量$\\sum _{u,(s,u)\\in E}f(u,v)$最大。 解析： 对最大流问题进行线性规划建模，可将总流量最大设为线性规划的优化目标。线性规划的约束包括两部分： 进入某个节点的流量之和等于离开某个节点的流量之和； 每条边的流量不小于零且不大于最大容量。 可得线性规划方程： $$ max\\; z=\\sum _{v,(s,v)\\in E}f(s,v) \\\\ \\begin{align*} s.t.\\; \\sum _{u,(u,v)\\in E}f(u,v)-\\sum _{w,(v,w)\\in E}f(v,w)\u0026=0,\u0026v\\in V-s \\\\ f(u,v)\u0026\\geq 0,\u0026(u,v)\\in E \\\\ f(u,v)\u0026\\leq C(u,v),\u0026(u,v)\\in E \\end{align*} $$ ","date":"2023-01-10","objectID":"/cs-lp/:2:0","tags":["个人笔记"],"title":"算法问题的线性规划建模","uri":"/cs-lp/"},{"categories":["个人笔记"],"content":"3 最小费用流问题（Minimum Cost Flow Problem） 输入： 一个有向图$G\u003cV,E\u003e$，每条边$e=(u,v)$与容量$C(u,v)$、每单位流量花费$a(u,v)$相对应，流量总量为$d$，源节点为$s$，目标节点为$t$。 输出： 求解每条边$e=(u,v)$对应的流量$0\\leq f(u,v)\\leq C(u,v)$，使到达目标节点的总流量为$d$，且总花费$\\sum _{(u,v)\\in E}a(u,v)f(u,v)$最小。 解析： 对最小费用流问题进行线性规划建模，可将费用最小设为线性规划的优化目标。线性规划的约束包括三部分： 进入某个节点的流量之和等于离开某个节点的流量之和； 每条边的流量不小于零且不大于最大容量； 总流量为$d$。 可得线性规划方程： $$ min\\; z=\\sum _{(u,v)\\in E}a(u,v)f(u,v) \\\\ \\begin{align*} s.t.\\; \\sum _{u,(u,v)\\in E}f(u,v)-\\sum _{w,(v,w)\\in E}f(v,w)\u0026=0,\u0026v\\in V-s \\\\ f(u,v)\u0026\\geq 0,\u0026(u,v)\\in E \\\\ f(u,v)\u0026\\leq C(u,v),\u0026(u,v)\\in E \\\\ \\sum _{v,(s,v)\\in E}f(s,v)\u0026=d \\end{align*} $$ ","date":"2023-01-10","objectID":"/cs-lp/:3:0","tags":["个人笔记"],"title":"算法问题的线性规划建模","uri":"/cs-lp/"},{"categories":["个人笔记"],"content":"4 多物品流问题（Multi Commodity Flow Problem） 输入： 一个有向图$G\u003cV,E\u003e$，每条边$e=(u,v)$对应容量$C_e$。共有$k$个物品，对第$i$个物品，$s_i,t_i,d_i$分别表示其起点、终点和总量。 输出： 满足多物品流量约束、边容量约束的一个可行解。 解析： 由于只需要求一个可行解，因此优化目标可直接设为$max;z=0$。线性规划的约束包括三部分： 进入某个节点的各物品流量之和等于离开某个节点的各物品流量之和； 每条边的各物品流量不小于零且不大于最大容量； 对于物品$i$，总流量为$d_i$。 设每条边$e$流经的第$i$个物品的流量为$f_i(u,v)$，可得线性规划方程： $$ max\\; z=0 \\\\ \\begin{align*} s.t.\\; \\sum _{u,(u,v)\\in E}f_i(u,v)-\\sum _{w,(v,w)\\in E}f_i(v,w)\u0026=0,\u0026v\\in V-s \\\\ f_i(u,v)\u0026\\geq 0,\u0026(u,v)\\in E \\\\ \\sum _{i=1}^k f_i(u,v)\u0026\\leq C(u,v),\u0026(u,v)\\in E \\\\ \\sum _{v,(s_i,v)\\in E}f(s_i,v)\u0026=d_i \\end{align*} $$ 线性规划是已知的该问题唯一多项式复杂度的解法。 ","date":"2023-01-10","objectID":"/cs-lp/:4:0","tags":["个人笔记"],"title":"算法问题的线性规划建模","uri":"/cs-lp/"},{"categories":["个人笔记"],"content":"5 SAT问题 SAT问题的介绍可参考此文章：SAT问题简介 概括而言，SAT问题的求解是为一个n元合取范式的变量进行真值指派，使得该式为真。 输入： 一个有m个子句的n元合取范式。 输出： 一组使该式为真的真值指派。 例题：$\\Phi=(x_1\\vee \\neg x_2 \\vee x_3)\\wedge (\\neg x_1\\vee x_2\\vee \\neg x_3)\\wedge (x_1\\vee x_2 \\vee \\neg x_3)$ 解析： 设线性规划的优化目标为最大化为真的子句数量，设指示第$j$个子句是否为真的变量为$c_j$，可得线性规划方程： $$ \\begin{align*} max\\;z=\\;\u0026 c_1 \u0026+\u0026c_2 \u0026+c_3 \u0026 \\\\ s.t.\\;\u0026 x_1 \u0026+\u0026(1-x_2) \u0026+x_3 \u0026\\geq c_1 \\\\ \u0026(1-x_1)\u0026+\u0026x_2 \u0026+(1-x_3)\u0026\\geq c_2 \\\\ \u0026x_1\u0026+\u0026x_2\u0026+(1-x_3)\u0026\\geq c_3 \\\\ \u0026x_1,\u0026\u0026x_2,\u0026x_3\u0026=0/1 \\\\ \u0026c_1,\u0026\u0026c_2,\u0026c_3\u0026=0/1 \\end{align*} $$ 上式中，约束的左侧表示为真的文字的数量，当至少存在一个为真时，该子句的真值便允许取值为1。当且仅当$z=3$时原问题有解。 ","date":"2023-01-10","objectID":"/cs-lp/:5:0","tags":["个人笔记"],"title":"算法问题的线性规划建模","uri":"/cs-lp/"},{"categories":["个人笔记"],"content":"一 绪论 ","date":"2021-06-28","objectID":"/osnote/:1:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"OS的作用 用户与硬件之间的接口 管理计算机资源 抽象计算机资源 ","date":"2021-06-28","objectID":"/osnote/:1:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"OS的发展 单道批处理系统 用户程序交给监控程序，由监控程序控制作业一个接一个交给IO处理 CPU等待IO、内存浪费、资源浪费 多道批处理系统 当一个作业在等待IO时，处理器可以切换到另一个不在等待IO的作业 中断：中断机构（硬件）发出信号，CPU转而处理中断程序工作，完成后再返回原来的工作 通道：专门负责输入输出的硬件，支持CPU和IO并行执行 作业的四种状态： 提交、后备、运行、完成 三级调度 低级调度：内存与内存 中级调度：内存与交换区 高级调度：外存到内存 多道程序设计好处 CPU利用率 内存和IO设备利用率 系统吞吐量 缺点 周转时间长 无交互能力 分时系统、实时系统、微机操作系统 操作系统的特征 并发、共享、异步、虚拟 概念区分：并发与并行、同步与异步 ","date":"2021-06-28","objectID":"/osnote/:1:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"二 OS结构 ","date":"2021-06-28","objectID":"/osnote/:2:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"陷阱和中断 中断：外部的被迫中断 陷阱：程序的主动中断 ","date":"2021-06-28","objectID":"/osnote/:2:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"双模式 计算机硬件通过模式位表示当前为内核模式或用户模式，区分操作系统执行的任务和用户执行的任务。可能引起损害的机器指令为特权指令，只有内核模式才可以执行特权指令。模式概念可以扩展为超过两个模式。 ","date":"2021-06-28","objectID":"/osnote/:2:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"系统调用 当要执行系统调用时，硬件通常将它作为软件中断。控制通过中断向量转到操作系统的中断服务程序，并且模式位也设为内核模式。内核首先验证参数是否正确和合法，执行请求，最后控制返回到系统调用之后的指令。 ","date":"2021-06-28","objectID":"/osnote/:2:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"定时器 为了防止用户程序陷入死循环，或者不调用系统服务且不将控制返回给操作系统，可以使用定时器，设置指定周期后中断计算机。 ","date":"2021-06-28","objectID":"/osnote/:2:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"三 进程与线程 ","date":"2021-06-28","objectID":"/osnote/:3:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"进程的定义 一个具有一定独立功能的可并发执行的程序，在一个数据集合上的运行过程 ","date":"2021-06-28","objectID":"/osnote/:3:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"进程与程序 运行的动态实体与静态实体 ","date":"2021-06-28","objectID":"/osnote/:3:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"进程的状态 新建状态、就绪状态、运行状态、阻塞状态、终止状态 ","date":"2021-06-28","objectID":"/osnote/:3:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"挂起 挂起/解挂的原因：用户、进程、系统的需要 增加挂起状态后：活动就绪、静止就绪、活动阻塞、静止阻塞 ","date":"2021-06-28","objectID":"/osnote/:3:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"PCB PCB是用以记录与进程相关信息的主存区，是进程存在的唯一标志 PCB的组织方式 线性方式、链接方式、索引方式 进程切换 P0-\u003e系统中断-\u003e[保存PCB0-\u003e…-\u003e加载PCB1]-\u003eP1-\u003e系统中断-\u003e[保存PCB1-\u003e…-\u003e加载PCB0]-\u003eP0 ","date":"2021-06-28","objectID":"/osnote/:3:5","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"进程创建与终止 进程图：用于描述进程家族关系的有向树，箭头表示创建关系 进程创建： 申请空白PCB 为新进程分配资源 初始化PCB 将新进程插入就绪队列 进程终止： 根据终止进程标识符，找到对应PCB 若该进程正在执行，终止进程，设置调度标志为真 如果该进程具有子孙进程，则终止其所有子孙 回收所有资源，归还给其父进程或系统 将PCB移出原所在队列 ","date":"2021-06-28","objectID":"/osnote/:3:6","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"进程通信 协作进程需要一种进程间通信机制（Interprocess Communicatio，IPC）来允许进程相互交互数据与信息 进程通信类型 共享存储器系统 消息传递系统 直接通信：Send(Receiver, msg), Receive(Sender, msg) 间接通信（信箱）：Send(mailbox, msg), Receive(mailbox, msg) 消息缓冲队列通信机制：生产者-消费者 管道通信 管道：用于连接一个读进程和一个写进程，以实现它们之间通信的共享文件，又称为Pipe文件 ","date":"2021-06-28","objectID":"/osnote/:3:7","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"线程 是进程中的一个实体，是能被系统独立调度和分派的基本单位 基本状态：就绪、执行、堵塞 基本操作：派生、调度、阻塞、激活、结束 ","date":"2021-06-28","objectID":"/osnote/:3:8","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"四 进程调度 三级调度模型 高级调度（作业调度）：后备队列-\u003e内存（长程调度） 低级调度（进程调度）：就绪队列-\u003e运行（短程调度） 中级调度（进程调度）：内存\u003c-\u003e交换区 调度算法 先来先服务（FCFS） 短作业优先（SJF） - 抢占式/非抢占式 优先级调度 - 抢占式/非抢占式 时间片轮转（RR） 多级队列 不同队列调度算法可以不同 队列之间有优先级差异或时间片分配 多级反馈队列 进程可在队列之间移动 参数： 队列数量 每个队列的调度算法 用以确定何时升级到更高优先级队列的方法 用以确定何时降级到更低优先级队列的方法 用以确定进程在需要服务时将会进入哪个队列的方法 ","date":"2021-06-28","objectID":"/osnote/:4:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"五 进程同步 ","date":"2021-06-28","objectID":"/osnote/:5:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"Peterson算法 两个进程互相谦让 while (true) { flag[i] = TRUE; turn = j; while ( flag[j] \u0026\u0026 turn == j); CRITICAL SECTION flag[i] = FALSE; REMAINDER SECTION } ","date":"2021-06-28","objectID":"/osnote/:5:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"信号量机制 semaphore mutex=1; process_1（）{ while(true) { wait(mutex); critical section; signal(mutex); remainder section; } } ","date":"2021-06-28","objectID":"/osnote/:5:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"生产者-消费者 Semaphore muter=1, empty=N, full=0; Producer() { while (true) { produce item v; wait(empty); wait(mutex); b[in] = v; in = (in+1)%N; signal(mutex); signal(full); } } Comsumer() { while (true) { wait(full); wait(mutex); w = b[out]; out = (out+1)%N; signal(mutex); signal(empty); consume item w; } } ","date":"2021-06-28","objectID":"/osnote/:5:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"读者-写者问题 Semaphore mutex=1, db=1; int readercount=0; Reader() { while (true) { wait(mutex); readcount++; if (readcount == 1) { wait(db); } signal(mutex); Reading; wait(mutex); readcount--; if (readcount == 0) { signal(db); } signal(mutex); } } Writer() { while (true) { wait(db); writing; signal(db); } } ","date":"2021-06-28","objectID":"/osnote/:5:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"管程 条件变量：condition x, y; 每个条件变量都关联了一个队列，包含因该条件而阻塞的进程，对条件变量仅有的操作是wait()、signal() x.wait()： 将自己阻塞在等待队列中 唤醒一个等待者或者释放管程的互斥访问 x.signal()： 将等待队列中的一个线程唤醒 如果等待队列为空，则等同空操作 ","date":"2021-06-28","objectID":"/osnote/:5:5","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"哲学家就餐问题（管程） monitor DP { enum { THINKING, HUNGRY, EATING} state [5] ; condition self [5]; void pickup (int i) { state[i] = HUNGRY; test(i); if (state[i] != EATING) self [i].wait; } void putdown (int i) { state[i] = THINKING; // testing left and right neighbors test((i + 4) % 5); test((i + 1) % 5); } void test (int i) { if ( (state[(i + 4) % 5] != EATING) \u0026\u0026 (state[i] == HUNGRY) \u0026\u0026 (state[(i + 1) % 5] != EATING) ) { state[i] = EATING ; self[i].signal () ; } } initialization_code() { for (int i = 0; i \u003c 5; i++) state[i] = THINKING; } } ","date":"2021-06-28","objectID":"/osnote/:5:6","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"六 死锁 多个进程因竞争资源而造成的一种僵局，若无外力作用，这些进程将永远无法推进 ","date":"2021-06-28","objectID":"/osnote/:6:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"死锁发生的必要条件 互斥 非抢占 占有并等待 循环等待 ","date":"2021-06-28","objectID":"/osnote/:6:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"资源分配图 无环一定不死锁 单资源有环一定死锁 ","date":"2021-06-28","objectID":"/osnote/:6:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"死锁的处理方法 死锁预防 破坏占有并等待 方法一：必须获得全部资源才能运行 方法二：申请资源前必须释放自己已占有所有资源 破坏非抢占 方法一：进程申请某资源，有则分配，无则释放该进程所有资源并等待 方法二：进程申请某资源，有则分配，否则检测有该资源的另一进程，若该进程在等待则抢占，否则申请资源的进程等待（允许被抢占资源） 破坏循环等待 给资源编号 进程只能按递增顺序申请资源 如果需要同一资源多个实例需一次申请 申请编号更小资源时，需先将大编号资源全部释放 破坏互斥 临界资源不可能做到 死锁避免 系统的安全状态：Max, Allocation, Available 若存在一分配顺序使进程全部执行的安全序列，则认为是安全状态 银行家算法：Max, Allocation, Need, Available 死锁检测 单实例：分配图 多实例：银行家 死锁恢复 进程终止 终止全部死锁进程 一次终止一个 资源抢占 选择一个牺牲品使代价最小 回滚：将进程回滚到某安全状态 饥饿：防止进程饥饿，需要有限牺牲某进程 ","date":"2021-06-28","objectID":"/osnote/:6:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"七 存储管理 ","date":"2021-06-28","objectID":"/osnote/:7:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"相关概念和术语 逻辑地址（logical address）/相对地址：CPU所生成的地址，又称为虚拟地址（virtual address） 物理地址（physical address）/绝对地址：内存单元所看到的地址（加载到内存地址寄存器中的地址） 逻辑地址空间（logical address space）：由程序所生成的所有逻辑地址的集合称为逻辑地址空间 物理地址空间（physicaladdress space ）：与逻辑地址对应的所有物理地址的集合称为物理地址空间 重定位（relocation）：从逻辑地址（虚拟地址）到物理地址的映射称为重定位，由内存管理单元（MMU）完成 重定位寄存器：基址寄存器、限长寄存器 绑定：从一个地址空间到另一个地址空间的映射 ","date":"2021-06-28","objectID":"/osnote/:7:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"内存保护 基地址寄存器：最小的合法的物理内存地址 界限地址寄存器：指定范围的大小 内存空间保护的实现通过CPU硬件对在用户模式下产生的地址与寄存器的地址进行比较来完成 ","date":"2021-06-28","objectID":"/osnote/:7:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"用户程序处理过程 重定位 静态重定位：程序装入主存之前由编译/链接程序完成重定位，入主存可立即执行 动态重定位：程序入主存之前不进行重定位，入主后存执行到与地址相关项时，再进行重定位 装入方式 绝对装入：程序在编译时完成静态重定位，按绝对地址装入内存 可重定位装入：根据内存使用情况，将装入模块进行静态重定位后装入内存 动态运行时装入：装入程序将装入模块装入内存后，不将装入模块的逻辑地址转换为物理地址，在执行时用重定位寄存器进行动态重定位 链接方式 静态链接：程序运行之前，先将各目标模块以及所需库函数链接成一个完整的装入模块 装入时动态链接：将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的链接方式 容易共享：每个库程序的引用都有一个存根，用于指出如何定位适当的内存驻留库程序 运行时动态链接：将对某些目标模块的链接推迟到程序执行时才进行 ","date":"2021-06-28","objectID":"/osnote/:7:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"连续分配存储管理 单一连续分配 固定分区分配 内碎片 动态分区分配/可变分区分配 算法： 首次适应 循环首次适应 最佳适应 最差适应 ","date":"2021-06-28","objectID":"/osnote/:7:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"碎片 外碎片：内存中小块的空闲分区，无法单独满足任何一个进程的需求 内碎片：固定分区分配方式中存在的，已经分配给进程，但进程不会使用的空闲空间 紧缩、离散分配内存 ","date":"2021-06-28","objectID":"/osnote/:7:5","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"覆盖与交换技术 覆盖：进程内部区域的替换，主要用于早期操作系统 把程序划分为若干功能上相对独立的程序段，按照自身逻辑结构将那些不会同时执行的程序段共享同一块内存区域 程序段先保存在磁盘上，当有关程序段的前一部分执行结束，把后续程序段调入内存，覆盖前面的程序段 交换：进程之间的替换 当内存空间紧张时，系统将内存中某些进程暂时移到外存（盘交换区），把外存中某些进程换入内存中，替换移出进程原来的内存空间 这种技术是进程在内存与外存之间的动态调度，多用于分时系统中 中级调度：采用交换技术 ","date":"2021-06-28","objectID":"/osnote/:7:6","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"分页存储管理 分页（长度固定） 分页思想 内存物理地址空间按2n等分成页框/帧（frame），并从0连续编号：0,1,2,… 作业的逻辑地址空间按页框/帧大小等分成页，并从0连续编号：0,1,2,… 作业逻辑地址表示为：v=(p,d) 作业连续的页，可以分配不连续的页框/帧 系统设置页表PMT保存作业各页入内存后的情况，包含页号、页框号 设置一个页表地址寄存器，保存当前执行进程页表的起始地址和页表的长度 分段（长度可变） 段页式（分段管理虚存，分页管理实存） ","date":"2021-06-28","objectID":"/osnote/:7:7","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"地址变换 直接地址变换 借助页表、页表寄存器完成作业的逻辑地址（虚地址）到内存物理地址的变换 页表基址寄存器（Page-table base register，PTBR）页表长度寄存器（Page-table length register，PRLR） 具有快表的地址变换 增设若干具有并行查询能力的特殊高速缓冲寄存器（联想寄存器/快表），保存当前执行进程的部分页表 程序的局部性特征 ","date":"2021-06-28","objectID":"/osnote/:7:8","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"页表 页表是一种特殊的数据结构，放在系统空间的页表区，存放逻辑页与物理页帧的对应关系。 每一个进程都拥有一个自己的页表，PCB表中有指针指向页表 层次页表 哈希页表 反置页表 一般页表：每个进程都有一个相关的页表，这些页表可能消耗大量的物理内存 反置页表：为每一个物理页框设置一个条目，并记录该物理页框所分配的进程ID和对应的逻辑页 ","date":"2021-06-28","objectID":"/osnote/:7:9","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"八 虚存管理 ","date":"2021-06-28","objectID":"/osnote/:8:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"请求分页机制 在分页系统的基础上增加了请求调页功能和页面置换功能 整个用户程序驻留在磁盘（二级存储）上，只装入少量页面入内存即可启动运行 程序运行中，由调页程序（pager）根据需要将单个页面 从磁盘调入内存， 将不需要的页面换出到磁盘上 置换时以页为单位 硬件支持 请求页表机制 缺页中断机构 地址变换机构 请求页表机制 状态位P（有效-无效位）：用于指示该页是否已经调入内存 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，以供置换算法在选择置换页面时参考 修改位M（脏位）：标识该页在调入内存后是否被修改过 外存地址：该页在外存上的地址，一般指物理块号 缺页中断机构处理过程 访问进程页表，查看访问页是否合法 如果是非法访问，则终止进程访问；如果页面访 问合法，但该页不在内存中，则需要调入 在物理内存中找到一个空闲页框（free frame） 调度磁盘操作，将失效页从备用存储（磁盘）中 装入空闲页框 重置页表，修改页面信息 重启因缺页而中断的指令 地址变换机构（有快表） 存储保护检查：页号\u003e页表长度？ 是，越界中断 否则2 查快表 找到，修改访问位对于写操作置修改位，形成物理地址访问 若未找到，查页表状态位 在主存，将表目写入快表 否则，缺页中断 ","date":"2021-06-28","objectID":"/osnote/:8:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"页面置换算法 基本页面置换流程 查找所需页在磁盘上的位置 查找一个空闲的页框： 如果有空闲页框，那么就使用它 如果有没有空闲页框，就使用页面置换算法以选 择一个“牺牲”页框（victim frame） 将“牺牲”页框的内容写到磁盘上，修改页表和 页框表 将所需页读入空闲页框，修改相应的页表和页 框表 重启用户进程 页面置换算法详解 评价指标：缺页率 先进先出（FIFO）页面置换算法 最佳（Optimal）置换算法（理论上） 置换未来最长时间内不会访问的页面 最近最少使用（LRU）页面置换算法 近似LRU页面置换算法 附加访问位算法 为页表中的每一页保留一个8位的字节 在规定的时间间隔（如每隔100ms）内，时钟定时器产生中断，OS将页面的访问位转移到8位字节的高位，其他位均向右移一位，最低位丢弃，这个8位字节记录了该页面在最近的8个时间间隔内的使用情况 选择值最小的页面置换，相等情况下按照FIFO算法执行 二次机会算法（Clock置换算法） 页表中的每一页有一个访问位（页面被访问后将其置为1） 按照FIFO的方式选择置换页，如果该页的访问位为1，则给予第二次机会，将其访问位置为0 最不经常使用（Least Frequently Used，LFU）页面置换算法 最常使用（Most Frequently Used，MFU）页面置换算法 具有最小次数的页可能刚刚调入内存 ","date":"2021-06-28","objectID":"/osnote/:8:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"页框分配算法 固定分配算法 平均分配 按比例分配：进程大小 基于优先级分配算法 按照进程的优先级成比例给进程分配页框 进程发生缺页中断时 选择自己的页框进行页面置换 选择优先级更低的进程的页框置换 全局分配与局部分配 全局置换：允许从所有页框中置换 局部置换：仅仅从分配给进程自己的页框中进行置换 ","date":"2021-06-28","objectID":"/osnote/:8:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"请求分页系统的性能分析 抖动（thrashing） CPU的利用率会随着道数的增加而提高，直到达到最大值 达到最大值后继续提高道数，CPU的利用率将不再提高，反而急剧下降，这就是“系统抖动” 为了提升CPU利用率，则需要减少道数，消除抖动 抖动产生的原因 同时在系统中运行的进程太多，分配给每个进程的页框太少，不能满足进程正常运行的基本要求 工作集：进程在某段时间内实际访问页的集合 $WS(t,\\Delta)$，$\\Delta$为工作集尺寸 $D=sum(WS_n(…))$，$m$为页框数，若$D\u003em$，则挂起进程 预防抖动 采取局部置换策略 将工作集算法融入作业调度 利用“L=S”准则调节缺页率 选择暂停的进程 ","date":"2021-06-28","objectID":"/osnote/:8:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"九 I/O系统 ","date":"2021-06-28","objectID":"/osnote/:9:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"I/O系统的基本功能 隐藏物理设备的细节 与设备的无关性 提高处理机和I/O设备的利用率 对I/O设备进行控制 确保对设备的正确共享 错误处理 ","date":"2021-06-28","objectID":"/osnote/:9:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"I/O系统的层次结构 用户层软件 设备独立性软件 设备驱动程序 中断驱动程序 硬件 ","date":"2021-06-28","objectID":"/osnote/:9:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"I/O系统接口 块设备接口 流设备接口 网络通信接口 ","date":"2021-06-28","objectID":"/osnote/:9:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"I/O系统结构 总线型结构 通道型结构 ","date":"2021-06-28","objectID":"/osnote/:9:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"I/O控制方式 程序直接控制方式 CPU轮询I/O设备 程序直接控制I/O方式中CPU直接控制I/O操作的过程 CPU的绝大部分时间都处于等待I/O设备完成数据I/O的循环测试中 中断驱动I/O控制方式 I/O控制器：一旦接受到CPU读命令，则开始读数据；完成读数据后发送中断信号表示已就绪，当CPU请求数据时传输数据，等待下一次I/O操作 CPU：CPU发出读信号以后继续其他工作，每个指令周期末尾检测中断信号，若检测到，则转向执行中断处理程序 DMA控制方式 数据传输的基本单位是数据块 所传送的数据是从设备直接送入内存的，或者相反 仅在传送一个或多个数据块的开始和结束时，才需CPU干预 通道控制方式 I/O通道控制方式是一种以内存为中心，实现外设与内存直接交换数据的控制方式 与DMA方式相比，通道所需要CPU干预更少， 每次可以完成多个不连续的数据块传送，而且可以做到一个通道控制多台设备，从而进一步减轻了CPU的工作 I/O通道具有自己的指令系统，并能实现指令所控制的操作，由CPU发出启动指令启动 通过执行通道程序并与设备控制器共同实现对I/O设备的控制 ","date":"2021-06-28","objectID":"/osnote/:9:5","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"缓冲 引入原因 缓和CPU与I/O设备间速度不匹配的矛盾 提高CPU和I/O设备之间的并行性 减少CPU对I/O的干预，减少对CPU的中断频率， 放宽对CPU中断响应时间的限制 缓冲实现机制 单缓冲区 双缓冲区 环缓冲区 缓冲池 对多个缓冲区的管理 收容输入、提取输入、收容输出、提取输出 设备独立性 逻辑设备与物理设备无关 SPOOLING（假脱机系统） 组成 输入井/输出井 输入缓冲区/输出缓冲区 输入进程/输出进程 当用户进程请求打印时， SPOOLing打印机系统为它做两件事： 在输出井中为之申请一个空闲磁盘分区， 并将要打印的数据送入其中 再为用户进程申请一张空白的用户请求打印表，并将用户的打印要求填入其中， 再将该表挂到打印请求队列上 打印机空闲时：输出进程取出一张打印请求表，再从输出井中取出打印数据到输出缓冲区，通过打印机进行打印 ","date":"2021-06-28","objectID":"/osnote/:9:6","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"十 文件系统 ","date":"2021-06-28","objectID":"/osnote/:10:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"文件的逻辑结构 有结构文件（记录式文件） 顺序文件 索引文件 索引顺序文件 直接文件/哈希文件 无结构文件（流式文件） ","date":"2021-06-28","objectID":"/osnote/:10:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"文件的目录结构 单级目录结构 两级目录结构：系统目录+用户目录 树形目录结构：在两级目录的基础上，允许用户自行添加子目录 无环图目录结构：同一文件和子目录可出现在不同目录中 别名问题、悬挂指针问题 通用目录结构：允许存在环 ","date":"2021-06-28","objectID":"/osnote/:10:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"文件共享 基于索引节点的共享方式 设置索引节点，存储文件的物理地址、链接计数（共享计数）及其它文件属性 文件目录只包括文件名和该文件对应索引节点的指针 利用符号链实现文件共享 假设B为了共享C的文件F，在B中创建一个Link类型的新文件，新文件目录中只包含被链接文件F的路径名，称这种链接方法为符号链接（symbolic Linking） 说明：只有文件主人的目录中有文件索引节点的指针，其它用户目录中只有路径名 ","date":"2021-06-28","objectID":"/osnote/:10:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"文件保护措施：存取控制 访问矩阵 ","date":"2021-06-28","objectID":"/osnote/:10:4","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"文件的物理结构 连续文件 链接文件 索引文件 ","date":"2021-06-28","objectID":"/osnote/:10:5","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"空闲空间管理 空闲表法 位向量 空闲块链 ","date":"2021-06-28","objectID":"/osnote/:10:6","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"十一 磁盘管理 ","date":"2021-06-28","objectID":"/osnote/:11:0","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"磁盘的访问时间 数据传输时间、平均旋转延迟时间、寻道时间 ","date":"2021-06-28","objectID":"/osnote/:11:1","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"磁盘调度算法 先来先服务（FCFS） 最短寻道优先（SSTF） 扫描算法（SCAN、电梯算法） 循环扫描（C-SCAN） LOOK/C-LOOK（不扫描到终点，到最远的进程就马上掉头） FSCAN：在扫描的过程中所有新产生的序列放在另外的一个队列中，当访问完当前队列之后，再访问新产生的一个队列；队列按SCAN算法处理 ","date":"2021-06-28","objectID":"/osnote/:11:2","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"廉价磁盘冗余阵列RAID 并行交叉存取： 有多台磁盘驱动器，系统将每一盘块中的数据分为若干个子盘块数据，再把每一个子盘块的数据分别存储到各个不同磁盘 中的相同位置上 当要将一个盘块的数据传送到内存时，采取并行传输方式，从而减少传输时间 ","date":"2021-06-28","objectID":"/osnote/:11:3","tags":["个人笔记"],"title":"操作系统复习笔记","uri":"/osnote/"},{"categories":["个人笔记"],"content":"1 编译与预编译版本选择 如果对LLVM没有特别需求，只是当作一般编译器使用，安装预编译版也足以应付场面；如果需要对LLVM作个性化定制，或基于LLVM开发，或学习LLVM源码，那手动编译LLVM工程会是更好的选择。 2 操作系统选择 此处选择Linux系统。 3 编译安装指南 LLVM是一个仍在快速发展的开源项目，已经成为又一个版本怪物，中文互联网上鱼龙混杂的各种LLVM编译步骤指南大多已经过时，或隐藏着各种未知的坑，即使是官方不同出处的文档，也可能存在各自不规整统一之处。 因此，LLVM编译指南靠谱程度依次为：GitHub仓库的Readme.md$\u003e$官方各种文档$\u003e$互联网其他编译指南。而我的建议是—— 直接从GitHub仓库克隆工程到本地进行编译！ 直接从GitHub仓库克隆工程到本地进行编译！ 直接从GitHub仓库克隆工程到本地进行编译！ 为什么直接从GitHub仓库clone？一方面，它足够完整，便于利用cmake指令灵活定制并集中编译；另一方面，GitHub上的Readme.md提供的安装指南一定是紧跟版本的，无过时之虞。如果不需要GitHub上LLVM的最新版本，只需要在把工程clone到本地以后再手动回退到指定tag的版本即可。 GitHub回退到指定版本（指定Tag）的网上教程很多，此处不再赘述。注意我们只需要硬回退本地版本，不需要回退以后提交到远程仓库的步骤。 综上所述，无痛编译安装LLVM的步骤和方法是：从GitHub仓库克隆工程到本地，将本地工程回退到指定版本，再按照对应版本的readme.md教程编译LLVM工程。 4 Linux下编译指定版本LLVM实例 编译平台：Ubuntu 20.04.2 LTS LLVM版本：10.0.1 LLVM的GitHub仓库链接：https://github.com/llvm/llvm-project 注：编译过程中需要的git, cmake等组件从略，请根据官方指示或编译中的实际踩坑安装所需依赖。 ","date":"2021-02-26","objectID":"/llvmsetup/:0:0","tags":["个人笔记"],"title":"Linux系统无痛编译安装LLVM简明指南","uri":"/llvmsetup/"},{"categories":["个人笔记"],"content":"4.1 仓库下载 国内下载GitHub仓库往往有速度太慢的问题，互联网对此有相当多的解决方案。此处采用的是CSDN博客https://blog.csdn.net/haejwcalcv/article/details/108028245 中提到的办法，具体地说，只需把github.com换成github.com.cnpmjs.org即可。 克隆仓库命令如下： git clone https://github.com.cnpmjs.org/llvm/llvm-project.git 注意，如果是在win平台下克隆仓库，还应当禁止代码换行符的自动转换，加上--config core.autocrlf=false： git clone --config core.autocrlf=false https://github.com.cnpmjs.org/llvm/llvm-project.git ","date":"2021-02-26","objectID":"/llvmsetup/:1:0","tags":["个人笔记"],"title":"Linux系统无痛编译安装LLVM简明指南","uri":"/llvmsetup/"},{"categories":["个人笔记"],"content":"4.2 回退版本 因为GitHub仓库维护的是最新版本，因此我们需要用git将版本回退到指定tag。首先，我们需要先查询llvm的tag列表： git tag 控制台输出如下： llvmorg-1.0.0 llvmorg-1.1.0 llvmorg-1.2.0 llvmorg-1.3.0 llvmorg-1.4.0 llvmorg-1.5.0 llvmorg-1.6.0 llvmorg-1.9.0 llvmorg-10-init llvmorg-10.0.0 llvmorg-10.0.0-rc1 llvmorg-10.0.0-rc2 llvmorg-10.0.0-rc3 llvmorg-10.0.0-rc4 llvmorg-10.0.0-rc5 llvmorg-10.0.0-rc6 llvmorg-10.0.1 llvmorg-10.0.1-rc1 llvmorg-10.0.1-rc2 llvmorg-10.0.1-rc3 llvmorg-10.0.1-rc4 llvmorg-11-init llvmorg-11.0.0 : 查看控制台输出可得，llvm 10.0.1对应的tag版本是llvmorg-10.0.1。查询该tag版本的信息指令如下： git show llvmorg-10.0.1 控制台输出如下： tag llvmorg-10.0.1 Tagger: Tom Stellard \u003ctstellar@redhat.com\u003e Date: Mon Jul 20 16:55:03 2020 -0700 Tag 10.0.1 commit ef32c611aa214dea855364efd7ba451ec5ec3f74 (HEAD -\u003e main, tag: llvmorg-10.0.1-rc4, tag: llvmorg-10.0.1, origin/release/10.x) Author: Hubert Tong \u003chubert.reinterpretcast@gmail.com\u003e Date: Thu Apr 30 22:18:54 2020 -0400 [tests] Revert unhelpful change from d73eed42d1dc (cherry picked from commit 0e8608b3c38886c224d252c6b126c804645b7761) diff --git a/llvm/test/CodeGen/X86/asm-modifier.ll b/llvm/test/CodeGen/X86/asm-modifier.ll index 0b8b240f7f7d..47b185a15766 100644 --- a/llvm/test/CodeGen/X86/asm-modifier.ll +++ b/llvm/test/CodeGen/X86/asm-modifier.ll @@ -1,4 +1,4 @@ -; RUN: llc \u003c %s -mtriple=x86_64-unknown-unknown | FileCheck %s +; RUN: llc \u003c %s | FileCheck %s : 由输出的内容可得，tag llvmorg-10.0.1对应的commit编号是ef32c611aa214dea855364efd7ba451ec5ec3f74，使用reset指令回退版本到该处： git reset --hard ef32c611aa214dea855364efd7ba451ec5ec3f74 版本回退成功后，输入git show可以看到，git仓库的head指针已经回退到了llvm 10.0.1版本。 ","date":"2021-02-26","objectID":"/llvmsetup/:2:0","tags":["个人笔记"],"title":"Linux系统无痛编译安装LLVM简明指南","uri":"/llvmsetup/"},{"categories":["个人笔记"],"content":"4.3 编译llvm工程 选择github仓库上的release/10.x分支，查阅对应的Readme.md，并根据Readme的指示进行安装。 首先，建立并切换到对应的build目录： cd llvm-project mkdir build cd build 然后，根据编译需求，输入cmake指令build工程。此处选择Ninja方式建立文件，额外编译clang, clang-tool-extra, compiler-rt项目。 cmake -G \"Ninja\" -DLLVM_ENABLE_PROJECTS=\"clang;clang-tools-extra;compiler-rt;\" ../llvm 在build的过程中，可能会出现一些Not found的提示信息，只要没有报错，就可以正常进行。 命令执行完以后，可以立刻输入echo $?命令查看上一条命令的执行情况，如果返回值为0，则说明build命令执行成功。 因为选择了Ninja方式build文件，此处采用ninja的命令完成工程构建： ninja \u0026\u0026 ninja install 同理，命令执行完以后，立刻输入echo $?命令查看上一条命令的执行情况，如果返回值为0，则说明build命令执行成功。 输入指令clang -v验证是否安装成功，返回如下信息： clang version 10.0.1 (https://github.com.cnpmjs.org/llvm/llvm-project.git ef32c611aa214dea855364efd7ba451ec5ec3f74) Target: x86_64-unknown-linux-gnu Thread model: posix InstalledDir: /usr/local/bin Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9 Selected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9 Candidate multilib: .;@m64 Selected multilib: .;@m64 可见已成功安装llvm+clang 10.0.1版本。 20210518更新：如安装过程中gg，有可能是交换空间不足，需要手动挂载swap，可以搜索引擎查找相关教程，本文不再列出；如果是其他错误，按错误提示按部就班解决即可。 ","date":"2021-02-26","objectID":"/llvmsetup/:3:0","tags":["个人笔记"],"title":"Linux系统无痛编译安装LLVM简明指南","uri":"/llvmsetup/"},{"categories":["个人笔记"],"content":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法 语法分析有两个总的思路，一个是自顶向下分析，一个是自底向上分析。自底向上的分析思路是，对一个句子$s$，不断进行归约（“合并”），看能否归约成开始符号$S$的状态。 ","date":"2021-01-27","objectID":"/compilerlr/:0:0","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"自底向上分析（LR概述） 自底向上分析通常讨论的是LR分析算法，也叫“移进-归约算法”。仍然是循序渐进的讨论，从比较朴素的归约动机开始，逐步讨论如何对其完善。 LR分析指每次从左(L)读入，从右(R)反向构造出最右推导序列。分析是在从左到右读入的过程中进行的。在读入一定子串以后，如果右端可以进行归约，则向上归约。引入点记号“·”以指示句子读入的情况。如下例子所示： 有文法如下： E -\u003e E + T | T T -\u003e T * F | F F -\u003e n 给出算式 2 + 3 * 4，归约过程如下： 2 + 3 * 4 2 · + 3 * 4 F · + 3 * 4 T · + 3 * 4 E · + 3 * 4 E + · 3 * 4 E + 3 · * 4 E + F · * 4 E + T · * 4 E + T * · 4 E + T * 4 · E + T * F · E + T · E · S · 由此，LR过程实际上生成的是一个逆序的最右推导。已处理部分的符号串，实际上是一个栈结构。因此，该算法的框架可以归纳成如下的移进、归约两个步骤。 移进：将一个记号移进到栈顶中； 归约：将栈顶的n个符号归约成一个非终结符。 显然，我们需要一个机制去确定何时应当移进，何时应当归约，否则该算法也会退化成盲目的搜索。 讨论“句柄”的概念。句柄在LR算法中可以理解为和产生式右部匹配的子串。显然，找到合适的句柄，我们就能在正确的步骤归约，从而实现对句子$s$的正确验证。 为了正确高效地实现LR算法，我们可以采取类似于LL的**表驱动的算法思路。**通常来说，LR类算法的分析表结构如下： 状态 ACTION GOTO …（非终结符分栏） …（终结符分栏） $ACTION$，即“动作”。在第$n$个状态时读入某终结符应当做什么动作。$ACTION$表的元素通常有2种： $si$，s是“shift”的缩写，指示状态的转换，i指示转换到的状态编号； $ri$，r是“reduce”的缩写，指示此时应当对栈顶子串做归约动作，i指示对应的（右部）表达式的编号。 此外，$ACTION$表中的“acc”是“accept”的缩写，表示读入到此处的时候可以已可以接受该句子，LR算法结束。 $GOTO$表意思是“跳转”，表示归约在栈顶得到某非终结符后，应该跳转到什么状态。 不同的LR算法，对分析表的构造方法并不相同。以下将对LR(0)，SLR，LR(1)，LALR进行讨论。 ","date":"2021-01-27","objectID":"/compilerlr/:1:0","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"LR(0)分析 首先，为求解方便，我们可以将文法$G$转换为增广文法$G’$，即对开始符号$S$增加一条$S’\\rightarrow S$，$S’$是新的开始符号，求解从$S’$开始。具体而言，这样做是为了开始符号只在位于一条式子的左部。 为讨论方便，补充项目的定义：项目即规约过程中的一个带点的句子，是左边规约部分和右边未读入部分的拼接。 那么，刚刚在讨论分析表时，表的每一横行代表某状态。所以，该如何划分状态呢？我们可以讨论一种“等价”。比如有式子 $ S\\rightarrow E+E,E\\rightarrow aBc $，当前的项目状态是$S\\rightarrow E+·E$。那么，我“期盼”着将要读入的是$E$，也就等价于我“期盼着”将要读入的是$aBc$。我们将这些等价的项目归为同一个集，称为项目集闭包，对应自动机中的一个状态。 由此，我们可以得到求项目集闭包的通俗描述：若”期盼着“读入某符号，将其归入一个状态；若该符号是非终结符，那么对于它的某右部表达式，也就相当于”期盼“着读入该表达式的第一个符号，那么该符号的相关项目也应当归入该状态。 根据构造闭包的思路，我们有LR(0)分析表构造算法如下： 首先，根据$S’\\rightarrow S$构造初始闭包C0，该闭包只含一个项目：$S’\\rightarrow ·S$ 若闭包的分析还没有完成，取出某还没有分析的闭包，根据它读入新符号以后的action和goto情况，建立新的状态闭包 LR(0)分析表构造算法伪代码如下： closure(C) // C是某状态对应的闭包 while (C is still changing) for (item i: C) // example: i = A-\u003e beta · B gamma C += {B -\u003e ...} Goto(C, x) // 求GOTO得到的新状态 tmp = {} for (item i: C) // example: i = A-\u003e beta · B gamma tmp += {A-\u003e beta B · gamma} return closure(C) LR0_table() C0 = closure(S'-\u003e·S$) state_set = {C0} Q = enqueue(C0) while (Q isn't empty) C = dequeue(Q) for (x: N+T) // N是非终结符集，T是终结符集 D = Goto(C, x) if (x in T) ACTION[C][x] = D else GOTO[C][x] = D if (D isn't in state_set) state_set.add(D) enqueue(D) LR(0)算法伪代码如下： stack = [] push($) // 终止符 push(1) // 初始状态 while (true) token t = nextToken() state s = stack[top] if (ACTION[s][t] == \"si\") push(t) push(i) else if (ACTION[s][t] == \"rj\") pop (the right hand of the production \"j:X-\u003ebeta\") state s = stack[top] push(X) push(Goto[s][X]) else error... ","date":"2021-01-27","objectID":"/compilerlr/:1:1","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"SLR分析 然而，LR(0)分析仍然可能存在冲突，考虑以下状态： E -\u003e T · T -\u003e T · * F 此时如果读入*，将无法得知应当采取移进动作还是采取归约动作（移进-归约冲突）。对于这种情况，SLR算法给出了一个依靠$FOLLOW$集解决的方案。对于下一个要读入的符号，它要么被移进，要么被归约成某个非终结符。设某状态下可接受若干符号，只要这些符号要么位于移进集合，要么位于某个FOLLOW集，集合之间两两互不相交，就可以用前看下一个输入的符号来解决。这种方法被称为简单的LR方法，也就是SLR方法。 SLR和LR的区别只在于对归约的处理： 对于状态i上的项目 X-\u003e ${\\alpha}$ · 仅对 y ${\\in}$FOLLOW(X)添加ACTION[i, y] ","date":"2021-01-27","objectID":"/compilerlr/:1:2","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"LR(1)分析 SLR分析仍然存在问题。SLR只是简单地考察下一个输入符号b是否属于与归约项目$A\\rightarrow\\alpha$相关联的$FOLLOW(A)$，但$b\\in FOLLOW(A)$只是归约$\\alpha$的一个必要条件，不是充分条件。LR(1)分析法提出的动机是，在特定位置，某符号$A$的后继符集合是$FOLLOW(A)$的子集。那么如何求出这个在特定位置的后继符集合呢？依靠前面已知状态的推导。观察如下例子： S -\u003e L = R L -\u003e * R 由第一条产生式可以进一步推导出 L -\u003e * R也应该加入到这个闭包里，但这两个L -\u003e * R是不一样的。原本的表达式是一个原始的式子，它期望后接的符号是终结符$，而由S推导出来的L式，后接的应该是=。因此推导出来的新闭包可以表示为： S -\u003e L = R, $ L -\u003e * R, = L -\u003e * R, $ 根据这样的思路，我们就把LR算法的前看1符号情况进行了细化，称为LR(1)算法。LR(1)的算法大部分和LR(0)相同，仅闭包的构造方法不同： 对项目$[X\\rightarrow\\alpha·Y\\beta,a]$ 添加$[Y\\rightarrow\\gamma,b]$到项目集 其中$b$是$X$式中$Y$被期望紧接着的符号（展望符）。 ","date":"2021-01-27","objectID":"/compilerlr/:1:3","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"LALR分析 LR(1)分析固然分析能力更强，但是也带来了新的问题：状态机中的许多状态，可能仅仅式被期望的后接符号（称为展望符）不一样，而他们的表达式集是完全相同的。LALR分析，即在LR(1)分析得到的状态机图基础上，将表达式完全相同的状态予以合并。这样合并以后，由于忽略了展望符的信息，可能产生归约-归约冲突。 LALR分析形式上与LR(1)相同，大小上与LR(0)/SLR相当，分析能力介于SLR和LR(1)之间。 分析能力：SLR(1)\u003cLALR(1)\u003cLR(1) 参考资料： 《编译原理》课程，陈鄞，哈尔滨工业大学（中国大学MOOC） 《编译原理》课程，华保健，中国科学院大学（网易云课堂或b站免费观看） 《编译原理》（紫龙书），机械工业出版社 《编译原理》，清华大学出版社 ","date":"2021-01-27","objectID":"/compilerlr/:1:4","tags":["个人笔记"],"title":"【编译原理】简明自底向上分析算法总结：LR(0)，SLR，LR(1)，LALR分析算法","uri":"/compilerlr/"},{"categories":["个人笔记"],"content":"语法分析概念 从编译器前端的流程上说，语法分析对词法分析得到的记号流进行分析，识别其中的语法错误，并将正确的记号流转化为语法树，交给编译器的后续步骤进行进一步处理。 ","date":"2021-01-27","objectID":"/compilerll/:0:0","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"上下文无关语法 上下文无关语法是一个四元组：$G=(T,N,P,S)$，其中 $T$是终结符集合 $N$是非终结符集合 $P$是一组产生式规则： 形式：$X\\rightarrow \\beta _1,\\beta _2,…,\\beta _n$ 其中$X\\in N,\\beta _i\\in (T\\cup N)$ $S$是唯一的开始符号，$S\\in N$ ","date":"2021-01-27","objectID":"/compilerll/:1:0","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"推导 给定文法$G$，从$S$开始，用产生式的右部替换左部的非终结符（把非终结符“展开”），直到不出现非终结符为止。推导结果称为句子。 最左推导：每次总选择最左侧符号替换 最右推导：每次总选择最右侧符号替换 语法分析的任务：给定$G$和句子$s$，回答是否存在对句子$s$的推导 ","date":"2021-01-27","objectID":"/compilerll/:2:0","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"分析树和二义性文法 ","date":"2021-01-27","objectID":"/compilerll/:3:0","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"分析树 推导可以表示成树状结构，树中的每个内部结点代表非终结符，每个叶子结点代表终结符，每一步推导代表如何从双亲结点生成直接孩子结点。 分析树的含义取决于树的后序遍历。 ","date":"2021-01-27","objectID":"/compilerll/:3:1","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"二义性文法 给定文法$G$，如果存在句子$s$对应不止一颗分析树，称$G$是二义性文法。 解决方案：文法重写 语法分析算法 显然，语法分析从两个思路去做： 思路1：根据文法$G$，从唯一的开始符号$S$开始，对非终结符，用产生式的右部替换左部（“展开”），观察是否能产生对应的句子$s$； 思路2：根据句子$s$，对其不断归约（“合并”），看是否能归约成开始符号$S$的形态。 思路1称为自顶向下分析，对应分析树的自顶向下的构造顺序；思路2称为自底向上分析。 ","date":"2021-01-27","objectID":"/compilerll/:3:2","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"自顶向下分析 从开始符号$S$出发去推导句子称为自顶向下分析。后文将从最原始的回溯展开动机出发，逐步讨论如何优化算法（然后更加秃头）。 ","date":"2021-01-27","objectID":"/compilerll/:4:0","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"朴素的回溯思路 最朴素的自顶向下分析思想是：从开始符号$S$出发，随意地推导出句子$t$，比较$t$和$s$。 以下是华保健老师里的网课伪代码： tokens[]; // holding all tokens i = 0; // 指向第i个token stack = [S] // S是开始符号 while (stack != []) if (stack[top] is a terminal t) if (t == tokens[i++]) // 如果匹配成功 pop(); else backtrack(); else if (stack[top] is a nonterminal T) pop(); push(the next right hand side of T) // 不符合，尝试下一个右部式 简单地说，就是不断地去试探展开式如何匹配每个句子，如果匹配不成功就回溯，试探下一种可能，由此引出递归下降分析算法和LL(1)分析算法。 ","date":"2021-01-27","objectID":"/compilerll/:4:1","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"递归下降分析算法 递归下降分析算法（需要回溯的方法）的基本思想如下： 每个非终结符构造一个分析函数 用前看符号指导产生式规则的选择 示例，对如下产生式规则： S -\u003e N V N N -\u003e s | t | g | w V -\u003e e | d 构造每条规则的算法伪代码： parse_S() parse(N) parse(V) parse(N) parse_N() token = token[i++] if (token == s || token == t || token == g || token == w) return; error(\"...\") parse_V() // TODO 一般的递归下降分析算法框架如下： parse_X() token = nextToken() switch(token) case ...: case ...: ...... 可以看出，通用的递归下降分析技术仍然可能需要回溯。据此进一步讨论LL(1)分析算法。 ","date":"2021-01-27","objectID":"/compilerll/:4:2","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"},{"categories":["个人笔记"],"content":"LL(1)分析 概念：从左(L)到右推导符号，最左(L)推导，采用一个(1)前看符号 基本思想：表驱动的分析方法 仍然是很直观的思路。对于需要回溯的情况，我们可以根据已有的条件和规律对其剪枝，从而避免无谓的搜索。而在语法分析当中，显然token与token之间的相对位置是存在关系的。据此，我们可以利用这些相对位置关系去构造分析表，记录遇到下一个符号时应该跳转到哪个状态，从而避免回溯。 具体地说，我们需要构造两个集合：$FIRST$集和$FOLLOW$集。$FIRST$集的动机是引导表达式展开的跳转方向，$FOLLOW$集的动机是避免空符$\\epsilon$带来的错误。当$FIRST$可能为空时，就应当加入$FOLLOW$集，后文算法将会体现这一点。 注：此处约定英文大写符号默认为非终结符，英文小写符号默认为终结符，希腊字母暗示为一般文法符号，无法确认是终结符还是非终结符。 $FIRST(\\alpha)$：从任意文法符号串$\\alpha$开始推导得出的所有可能的终结符集合。通俗地说，就是$\\alpha$可能以什么开头。 $FOLLOW(A)$：可能在某些句型中紧跟$A$后边的终结符号的集合。 对$FIRST$集可以采用不动点算法计算。计算各文法符号的$FIRST(\\alpha)$时，不断应用下列规则刷表，直到所有FIRST()都不再更新： 如果$\\alpha=a$是终结符，则$FIRST(a)={a}$； 如果$\\alpha=N$不是终结符，则对$N$的每个产生式$\\beta_1\\beta_2…\\beta_n$，对其中的$\\beta_i$，若$\\beta_1\\beta_2…\\beta_{i-1}$的$FIRST$集中都有$\\epsilon$空符，则$FIRST(N) \\cup = FIRST(\\beta_i)$。通俗地说，对于产生式的第$i$个符号，如果它前面的符号都可能取空，那么它的$FIRST$元素当然也可能是$N$的$FIRST$元素，因此要将它的集合加入到$N$的集合中去。当然，对于$i=1$的情况，总是会$FIRST(N)\\cup FIRST(\\beta_1)$。 给出伪代码如下： while (some set is changing) for (symbol alpha: symbols) if (alpha is terminal) FIRST[alpha] = {alpha} else if (alpha is nonterminal) for (production p: alpha-\u003ebeta_1,...beta_n) for (i = 1; i \u003c= n; i++) if (beta_i is terminal a) FIRST[alpha] += {a} break else if (beta_i is nonterminal M) FIRST[N] += FIRST[M] if (M cannot be null) break 类似地，可以给出$FOLLOW$集的不动点算法： 对所有非终结符$A$的FOLLOW(A)集合时，不断应用以下规则刷表，不再有集合被更新： 3. 如果存在产生式$A\\rightarrow \\alpha B\\beta$，则$\\beta$除空符$\\epsilon$以外的所有符号都在$FOLLOW(B)$中； 4. 如果存在产生式$A\\rightarrow \\alpha B$，或$A\\rightarrow \\alpha B\\beta$且$FIRST(\\beta)$包含空符$\\epsilon$，则$FOLLOW(B) \\cup=FOLLOW(A)$。 通俗地说，就是找到每个“紧跟其后”的终结符号。类比$FIRST$集的求法，把“紧跟其后”的符号的FIRST集加入所求的FOLLOW集中。如果“紧跟其后”的$FIRST$集有机会取空符，那么下一个集合的$FIRST$集合也有机会补上来“紧跟其后”，以此类推。 伪代码如下： while (some set is changing) for (nonterminal A : nonterminals) for (production p: A-\u003ebeta_1,...,beta_n) tmp = FOLLOW[N] for (i = n; i \u003e 0; i--) if (beta_i == terminal a) tmp = {a} else if (beta_i == nonterminal M) FOLLOW[M] += tmp if (M cannot be null) tmp = FIRST[M] else tmp += FIRST[M] 已有$FIRST集$，$FOLLOW$集，可按如下规则构造二维预测分析表$M[][]$，该预测分析表的行头是非终结符号，列头是输入的下一个符号（终结符号）。对文法$G$的每个产生式$A\\rightarrow\\alpha$，进行如下处理： 对于$FIRST(\\alpha)$的每个终结符号$\\alpha$，将$A\\rightarrow\\alpha$加入到$M[A,a]$中 如果$\\epsilon\\in FIRST(\\alpha)$，则对于$FOLLOW(A)$的每个终结符号$b$，也将$A\\rightarrow\\alpha$加入到$M[A,b]$中。 由上可以很清楚地看出$FOLLOW$集的作用：前一个$FIRST$为空的时候的“替补”。通过同时求解$FIRST$集和$FOLLOW$集，可以保证LL(1)分析总能前看到“正确”的符号。 由上，可以书写LL(1)分析伪代码： tokens[]; // all tokens i = 0; stack = [S] // S是开始符号 while (stack != []) if (stack[top] is a terminal t) if (t == token[i++]) pop(); else error(...); // 朴素自顶向下的回溯改成直接报错 else if (stack[top] is a nonterminal T) pop(); push(table[T, tokens[i]]); // 朴素自顶向下的尝试展开任意表达式改成按表展开 尽管LL(1)对朴素自顶向下做了优化，但LL(1)在语法上仍然存在发生冲突的可能。下面简单讨论两种解决冲突的方法：消除左递归和提取左公因子。 消除左递归 有例子如下： E -\u003e E + T | T 此时由于LL算法总是从左到右读入，从左到右展开，那么计算的时候将会无限展开E-\u003eE+T而进入死循环。对于左递归的情况，有一般解法可以转换成右递归，如上式可改写如下： E -\u003e T E' E'-\u003e + T E' 提取左公因子 有例子如下： X -\u003e a Y | a Z 显然，此时同样的a可能会导向不同的表达式，存在冲突，直观的解决方法时将公共的a提取出来，如下所示： X -\u003e a X' X'-\u003e Y | Z 先整理自顶向下到这里，有空再整理自底向上…… 参考资料： 《编译原理》课程，华保健，中国科学院大学（网易云课堂或b站免费观看） 《编译原理》（紫龙书），机械工业出版社 《编译原理》，清华大学出版社 ","date":"2021-01-27","objectID":"/compilerll/:4:3","tags":["个人笔记"],"title":"【编译原理】简明自顶向下分析算法总结：递归下降，LL(1)分析算法","uri":"/compilerll/"}]