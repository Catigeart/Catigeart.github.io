<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - Catigeart&#39;s Software Development Note</title>
        <link>http://catigeart.github.io/posts/</link>
        <description>All Posts | Catigeart&#39;s Software Development Note</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>catigeart@gmail.com (Catigeart)</managingEditor>
            <webMaster>catigeart@gmail.com (Catigeart)</webMaster><lastBuildDate>Sun, 20 Aug 2023 15:01:00 &#43;0800</lastBuildDate><atom:link href="http://catigeart.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>生成实数随机可逆矩阵与随机正定矩阵（C&#43;&#43;）</title>
    <link>http://catigeart.github.io/positivedefinitematrix/</link>
    <pubDate>Sun, 20 Aug 2023 15:01:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/positivedefinitematrix/</guid>
    <description><![CDATA[由于最近手搓库函数，需要生成对称正定矩阵作为输入，因此上网查了下生成的思路，发现讨论者寥寥，即使有也不是c/cpp的底层实现，因此自己搓了个]]></description>
</item>
<item>
    <title>【论文笔记】Attention Is All You Need</title>
    <link>http://catigeart.github.io/attention-is-all-you-need/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/attention-is-all-you-need/</guid>
    <description><![CDATA[主要的序列转导模型是基于复杂的循环或卷积神经网络，包括一个编码器和一个解码器。表现最好的模型还通过注意机制连接编码器和解码器。我们提出了一个]]></description>
</item>
<item>
    <title>【论文笔记】Deep Forest - Towards an alternative to deep neural networks</title>
    <link>http://catigeart.github.io/deep-forest-towards-an-alternative-to-deep-neural-networks/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/deep-forest-towards-an-alternative-to-deep-neural-networks/</guid>
    <description><![CDATA[目前的深度学习模型大多建立在神经网络上，即多层参数化的可微非线性模块，可以通过反向传播进行训练。在本文中，我们探索了基于不可微模块构建深度模]]></description>
</item>
<item>
    <title>【论文笔记】Deep Residual Learning for Image Recognition</title>
    <link>http://catigeart.github.io/deep-residual-learning-for-image-recognition/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/deep-residual-learning-for-image-recognition/</guid>
    <description><![CDATA[深度神经网络更难训练。我们提出了一个残差学习框架，以简化比以前使用的网络深度大得多的网络的训练。我们明确地将层重新表述为参考层输入的学习残差]]></description>
</item>
<item>
    <title>【论文笔记】Graph attention networks</title>
    <link>http://catigeart.github.io/graph-attention-networks/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/graph-attention-networks/</guid>
    <description><![CDATA[Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017. 我们提出了图注意网络(GATs)，这是一种新颖的神经网络架构，可以在图结构数据上运行，利用隐]]></description>
</item>
<item>
    <title>【论文笔记】Modeling Relational Data with Graph Convolutional Networks</title>
    <link>http://catigeart.github.io/modeling-relational-data-with-graph-convolutional-networks/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/modeling-relational-data-with-graph-convolutional-networks/</guid>
    <description><![CDATA[Schlichtkrull M, Kipf T N, Bloem P, et al. Modeling relational data with graph convolutional networks[C]//The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3–7, 2018, Proceedings 15. Springer International Publishing, 2018: 593-607. 知识图支持各种各样的应用，包括问题回答和信息检索。尽管在它们的]]></description>
</item>
<item>
    <title>【论文笔记】Neural Message Passing for Quantum Chemistry</title>
    <link>http://catigeart.github.io/neural-message-passing-for-quantum-chemistry/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/neural-message-passing-for-quantum-chemistry/</guid>
    <description><![CDATA[Gilmer J, Schoenholz S S, Riley P F, et al. Neural message passing for quantum chemistry[C]//International conference on machine learning. PMLR, 2017: 1263-1272. 分子的监督学习在化学、药物发现和材料科学方面具有不可思议的潜力。幸运的是，文献中已经描述了几个]]></description>
</item>
<item>
    <title>【论文笔记】Non-local Neural Networks</title>
    <link>http://catigeart.github.io/non-local-neural-networks/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/non-local-neural-networks/</guid>
    <description><![CDATA[Wang X, Girshick R, Gupta A, et al. Non-local neural networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7794-7803. 卷积运算和循环运算都是每次处理一个局部邻域的构建块。在本文中，我们将非局部操作作为捕获远程依赖]]></description>
</item>
<item>
    <title>【论文笔记】Recurrent neural network based language model</title>
    <link>http://catigeart.github.io/recurrent-neural-net-work-based-language-model/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/recurrent-neural-net-work-based-language-model/</guid>
    <description><![CDATA[提出了一种新的基于递归神经网络的语言模型(RNN LM)，并将其应用于语音识别。结果表明，与最先进的后退语言模型相比，使用几个RNN LMs的混]]></description>
</item>
<item>
    <title>【论文笔记】Reducing the dimensionality of data with neural networks</title>
    <link>http://catigeart.github.io/reducing-the-dimensionality-of-data-with-neural-networks/</link>
    <pubDate>Tue, 30 May 2023 00:38:00 &#43;0800</pubDate>
    <author>Catigeart</author>
    <guid>http://catigeart.github.io/reducing-the-dimensionality-of-data-with-neural-networks/</guid>
    <description><![CDATA[摘要 通过训练具有小中心层的多层神经网络重构高维输入向量，可以将高维数据转换为低维代码。梯度下降可以用于微调这种“自动编码器”网络中的权重，但]]></description>
</item>
</channel>
</rss>
